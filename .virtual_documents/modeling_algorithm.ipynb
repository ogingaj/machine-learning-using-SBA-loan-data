# Core Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.ticker as mtick
import seaborn as sns
import shap

# Preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split

# Models
from sklearn.svm import SVC, LinearSVC, LinearSVR
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from xgboost import XGBClassifier, XGBRegressor

# Evaluation Metrics
from sklearn.metrics import (
    mean_squared_error,
    r2_score,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    ConfusionMatrixDisplay
)

# Pipeline
from sklearn.pipeline import make_pipeline



#read pre-processed data 
df = pd.read_csv(r"C:\Users\Ochieng' Oginga\Documents\Post_S\Spring_2025\Data_Science_II\Project\Data\Oginga_Stage4.csv")

#show head 
df.head()





df.shape








#get description of the data 
df.describe(include='all')






#get summary of the numeric variables 
df.describe()





#summary of categorical variables 
df.describe(include=[object])








#create a histogram for bumeric variables with bins of 20 
df.hist(figsize=(12, 10), bins=20)





 # Ensure ApprovalFY is numeric
df['ApprovalFY'] = pd.to_numeric(df['ApprovalFY'], errors='coerce')

# Drop any NaN values that might have been introduced during conversion
df = df.dropna(subset=['ApprovalFY'])

# Aggregate and plot
df.groupby('ApprovalFY')['GrAppv'].sum().plot(kind='bar', figsize=(10, 5))

plt.xlabel("Approval Fiscal Year")
plt.ylabel("Approved Gross ($)")
plt.title("Total Approved Gross by ApprovalFY")
plt.xticks(rotation=45)
plt.show()








# Extract the last two digits (year) from DisbursementDate and convert to integer
df['DisbursementYear'] = df['DisbursementDate'].str[-2:].astype(int)

# Fix potential century issue: Convert 00-23 to 2000-2023, and 80-99 to 1980-1999
df['DisbursementYear'] = df['DisbursementYear'].apply(lambda x: x + 2000 if x < 24 else x + 1900)

# Aggregate total disbursement by year
yearly_disbursement = df.groupby('DisbursementYear')['DisbursementGross'].sum()

# Plot
fig, ax = plt.subplots(figsize=(10, 5))
yearly_disbursement.plot(kind='bar', ax=ax)

# Formatting
ax.set_xlabel("Disbursement Year")
ax.set_ylabel("Total Loan Disbursement ($)")
ax.set_title("Total Loan Disbursement by Year")
ax.yaxis.set_major_formatter(mtick.StrMethodFormatter('${x:,.0f}'))  # Format as dollars
plt.xticks(rotation=45)

plt.show()








#box plotting
plt.figure(figsize=(12, 6))
sns.boxplot(data=df.select_dtypes(include=['number']))
plt.xticks(rotation=90)
plt.show()









#check columns 
df.columns








#set features and target 
features = ['Term', 'NoEmp', 'NewExist', 'CreateJob', 'RetainedJob', 'DisbursementGross', 'ChgOffPrinGr', 'SBA_Appv', 
            'ChgOffDate_binary', 'UrbanRural_binary', 'MIS_Status_Binary', 
            'IsFranchise', 'LowDoc_binary', 'RevLineCr_binary',
            'Sector_Accommodation & Food', 'Sector_Admin & Waste Mgmt',
            'Sector_Agriculture', 'Sector_Arts & Recreation', 'Sector_Construction',
            'Sector_Education', 'Sector_Finance', 'Sector_Healthcare',
            'Sector_Information', 'Sector_Management', 'Sector_Manufacturing',
            'Sector_Mining', 'Sector_Other Services', 'Sector_Professional Services',
            'Sector_Public Admin', 'Sector_Real Estate', 'Sector_Retail Trade',
            'Sector_Transportation', 'Sector_Unknown', 'Sector_Utilities',
            'Sector_Wholesale Trade']

X = df[features]
y = df['GrAppv']

#Split into 80% train+val, 20% test
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

#Split train_val into 87.5% train, 12.5% val ( to achieve 70% train, 10% val)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, random_state=42)





#Scale features for standardization 
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Fit Linear Regression Model
lin_reg = LinearRegression()
lin_reg.fit(X_train_scaled, y_train)

# Predict
y_train_pred = lin_reg.predict(X_train_scaled)
y_val_pred = lin_reg.predict(X_val_scaled)
y_test_pred = lin_reg.predict(X_test_scaled)

# Evaluate
def evaluate(y_true, y_pred, label=""):
    mse = mean_squared_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    print(f"{label} MSE: {mse:.2f}, R²: {r2:.4f}")

evaluate(y_train, y_train_pred, "Train")
evaluate(y_val, y_val_pred, "Validation")
evaluate(y_test, y_test_pred, "Test")

#save predictions
y_test_pred_lr = y_test_pred


# Scatter plot of actual vs predicted
plt.figure(figsize=(6,6))
sns.scatterplot(x=y_test, y=y_test_pred)
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title("Actual vs Predicted Values")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # diagonal line
plt.show()





# Train Random Forest Regressor with scaled data
rf_model = RandomForestRegressor(
    n_estimators=100,
    max_depth=10,
    n_jobs=-1,
    random_state=42
)
# Use scaled features
rf_model.fit(X_train_scaled, y_train)  

# Predictions
y_train_pred_rf = rf_model.predict(X_train_scaled)
y_val_pred_rf = rf_model.predict(X_val_scaled)
y_test_pred_rf = rf_model.predict(X_test_scaled)

# Evaluate model performance
train_mse_rf = mean_squared_error(y_train, y_train_pred_rf)
val_mse_rf = mean_squared_error(y_val, y_val_pred_rf)
test_mse_rf = mean_squared_error(y_test, y_test_pred_rf)

train_r2_rf = r2_score(y_train, y_train_pred_rf)
val_r2_rf = r2_score(y_val, y_val_pred_rf)
test_r2_rf = r2_score(y_test, y_test_pred_rf)

# Print results
print(f"Random Forest Train MSE: {train_mse_rf:.4f}, R²: {train_r2_rf:.4f}")
print(f"Random Forest Validation MSE: {val_mse_rf:.4f}, R²: {val_r2_rf:.4f}")
print(f"Random Forest Test MSE: {test_mse_rf:.4f}, R²: {test_r2_rf:.4f}")

#save predictions
y_test_pred_rf = rf_model.predict(X_test_scaled)


# Scatter plot of actual vs predicted for Random Forest
plt.figure(figsize=(6,6))
sns.scatterplot(x=y_test, y=y_test_pred_rf)
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title("Random Forest: Actual vs Predicted Values")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # diagonal line
plt.show()







# Train XGBoost Regressor
xgb_model = XGBRegressor(n_estimators=50, random_state=42, eval_metric="rmse", n_jobs=-1)
xgb_model.fit(X_train, y_train)

# Predictions
y_train_pred = xgb_model.predict(X_train)
y_val_pred = xgb_model.predict(X_val)
y_test_pred = xgb_model.predict(X_test)

# Evaluate model performance
train_mse = mean_squared_error(y_train, y_train_pred)
val_mse = mean_squared_error(y_val, y_val_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

train_r2 = r2_score(y_train, y_train_pred)
val_r2 = r2_score(y_val, y_val_pred)
test_r2 = r2_score(y_test, y_test_pred)

# Print results
print(f"Train MSE: {train_mse:.4f}, R²: {train_r2:.4f}")
print(f"Validation MSE: {val_mse:.4f}, R²: {val_r2:.4f}")
print(f"Test MSE: {test_mse:.4f}, R²: {test_r2:.4f}")


#save predictions
y_test_pred_xgb = xgb_model.predict(X_test)



# Scatter plot of actual vs predicted for XGBoost
plt.figure(figsize=(6,6))
sns.scatterplot(x=y_test, y=y_test_pred)
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title("XGBoost: Actual vs Predicted Values")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # diagonal line
plt.show()






# Create a pipeline with StandardScaler, PCA, and LinearSVR
svr_pipeline = make_pipeline(
    StandardScaler(),
    PCA(n_components=0.95),  # Retain 95% of variance
    LinearSVR(random_state=42, max_iter=10000)
)

# Fit model on training data
svr_pipeline.fit(X_train, y_train)

# Predictions
y_train_pred = svr_pipeline.predict(X_train)
y_val_pred = svr_pipeline.predict(X_val)
y_test_pred = svr_pipeline.predict(X_test)

# Evaluate model performance
train_mse = mean_squared_error(y_train, y_train_pred)
val_mse = mean_squared_error(y_val, y_val_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

train_r2 = r2_score(y_train, y_train_pred)
val_r2 = r2_score(y_val, y_val_pred)
test_r2 = r2_score(y_test, y_test_pred)

# Print results
print(f"Train MSE: {train_mse:.4f}, R²: {train_r2:.4f}")
print(f"Validation MSE: {val_mse:.4f}, R²: {val_r2:.4f}")
print(f"Test MSE: {test_mse:.4f}, R²: {test_r2:.4f}")

#save predictions 
y_test_pred_svr = svr_pipeline.predict(X_test)


# Scatter plot of actual vs predicted for SVR
plt.figure(figsize=(6,6))
sns.scatterplot(x=y_test, y=y_test_pred)
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title("SVR: Actual vs Predicted Values")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # diagonal line
plt.show()



# Create a dictionary of results
results_apprv = {
    'Model': ['Linear Regression', 'Random Forest', 'XGBoost', 'Linear SVM (10%)'],
    'Train MSE': [1989391154.14, 129422729.8295, 1075209796.1439, 21029628772.2606],
    'Train R²': [0.9783, 0.9986, 0.9883, 0.7705],
    'Val MSE': [1794591167.47, 246892299.9305, 1834541327.1724, 20930006005.8791],
    'Val R²': [0.9802, 0.9973, 0.9798, 0.7696],
    'Test MSE': [1981584865.49, 259425575.0244, 1923854431.4823, 21209814478.5109],
    'Test R²': [0.9785, 0.9972, 0.9791, 0.7695]
}

# Convert to DataFrame
results_df = pd.DataFrame(results_apprv)

# Display it
results_df











# Initialize the SHAP explainer
explainer_rf = shap.TreeExplainer(rf_model)

# Calculate 5% of the training data size
sample_size = int(0.05 * X_train_scaled.shape[0])

# Randomly select 1% of indices
sample_indices = np.random.choice(X_train_scaled.shape[0], size=sample_size, replace=False)

# Subset scaled data
X_sample_scaled = X_train_scaled[sample_indices]

# Convert to DataFrame with feature names
X_sample_df = pd.DataFrame(X_sample_scaled, columns=X_train.columns)

# Compute SHAP values for the 1% sample
shap_values_rf = explainer_rf.shap_values(X_sample_scaled)

# Display the summary plot with actual feature names
shap.summary_plot(shap_values_rf, X_sample_df)








# Define features and target variable
features = ['NoEmp', 'NewExist', 'DisbursementGross', 'GrAppv', 'SBA_Appv', 'UrbanRural_binary', 'MIS_Status_Binary', 
            'IsFranchise', 'LowDoc_binary', 'RevLineCr_binary',
            'Sector_Accommodation & Food', 'Sector_Admin & Waste Mgmt',
            'Sector_Agriculture', 'Sector_Arts & Recreation', 'Sector_Construction',
            'Sector_Education', 'Sector_Finance', 'Sector_Healthcare',
            'Sector_Information', 'Sector_Management', 'Sector_Manufacturing',
            'Sector_Mining', 'Sector_Other Services', 'Sector_Professional Services',
            'Sector_Public Admin', 'Sector_Real Estate', 'Sector_Retail Trade',
            'Sector_Transportation', 'Sector_Unknown', 'Sector_Utilities',
            'Sector_Wholesale Trade']

X = df[features]
y = df['ChgOffPrinGr']

# Split data into 80% train+val and 20% test
X_train_val_prin, X_test_prin, y_train_val_prin, y_test_prin = train_test_split(X, y, test_size=0.20, random_state=42)

# Split train+val into 70% train and 10% val (which is 0.125 of 80%)
X_train_prin, X_val_prin, y_train_prin, y_val_prin = train_test_split(X_train_val_prin, y_train_val_prin, test_size=0.125, random_state=42)






# Standardize the features
scaler_prin = StandardScaler()
X_train_scaled_prin = scaler_prin.fit_transform(X_train_prin)
X_val_scaled_prin = scaler_prin.transform(X_val_prin)
X_test_scaled_prin = scaler_prin.transform(X_test_prin)

# Train Linear Regression model
lr_model_prin = LinearRegression()
lr_model_prin.fit(X_train_scaled_prin, y_train_prin)

# Predictions
y_train_pred_prin = lr_model_prin.predict(X_train_scaled_prin)
y_val_pred_prin = lr_model_prin.predict(X_val_scaled_prin)
y_test_pred_prin = lr_model_prin.predict(X_test_scaled_prin)

# Evaluate model performance
train_mse_prin = mean_squared_error(y_train_prin, y_train_pred_prin)
val_mse_prin = mean_squared_error(y_val_prin, y_val_pred_prin)
test_mse_prin = mean_squared_error(y_test_prin, y_test_pred_prin)

train_r2_prin = r2_score(y_train_prin, y_train_pred_prin)
val_r2_prin = r2_score(y_val_prin, y_val_pred_prin)
test_r2_prin = r2_score(y_test_prin, y_test_pred_prin)

# Print results
print(f"Train MSE: {train_mse_prin:.4f}, R²: {train_r2_prin:.4f}")
print(f"Validation MSE: {val_mse_prin:.4f}, R²: {val_r2_prin:.4f}")
print(f"Test MSE: {test_mse_prin:.4f}, R²: {test_r2_prin:.4f}")

# Plot Actual vs Predicted for test set
plt.figure(figsize=(6,6))
sns.scatterplot(x=y_test_prin, y=y_test_pred_prin)
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title("Actual vs Predicted Values (Linear Regression on Principal Components)")
plt.plot([y_test_prin.min(), y_test_prin.max()], [y_test_prin.min(), y_test_prin.max()], 'r--')  # diagonal line
plt.show()






# Train Random Forest model on _prin split
rf_model_prin = RandomForestRegressor(n_estimators=10, random_state=42)
rf_model_prin.fit(X_train_prin, y_train_prin)

# Predictions
y_train_pred_prin = rf_model_prin.predict(X_train_prin)
y_val_pred_prin = rf_model_prin.predict(X_val_prin)
y_test_pred_prin = rf_model_prin.predict(X_test_prin)

# Evaluate performance
train_mse_prin = mean_squared_error(y_train_prin, y_train_pred_prin)
val_mse_prin = mean_squared_error(y_val_prin, y_val_pred_prin)
test_mse_prin = mean_squared_error(y_test_prin, y_test_pred_prin)

train_rmse_prin = np.sqrt(train_mse_prin)
val_rmse_prin = np.sqrt(val_mse_prin)
test_rmse_prin = np.sqrt(test_mse_prin)

train_r2_prin = r2_score(y_train_prin, y_train_pred_prin)
val_r2_prin = r2_score(y_val_prin, y_val_pred_prin)
test_r2_prin = r2_score(y_test_prin, y_test_pred_prin)

# Print results
print(f"Train RMSE: {train_rmse_prin:.4f}, R²: {train_r2_prin:.4f}")
print(f"Validation RMSE: {val_rmse_prin:.4f}, R²: {val_r2_prin:.4f}")
print(f"Test RMSE: {test_rmse_prin:.4f}, R²: {test_r2_prin:.4f}")

#save predictions
y_test_pred_prin = rf_model_prin.predict(X_test_prin)



#plot actural versus prediced for train 
plt.figure(figsize=(6, 6))
sns.scatterplot(x=y_test_prin, y=y_test_pred_prin)
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Random Forest: Actual vs Predicted (Test Set)")
plt.plot([y_test_prin.min(), y_test_prin.max()],
         [y_test_prin.min(), y_test_prin.max()],
         'r--')
plt.show()






# Train XGBoost model on _prin split
xgb_model_prin = XGBRegressor(n_estimators=50, random_state=42, eval_metric="rmse", n_jobs=-1)
xgb_model_prin.fit(X_train_prin, y_train_prin)

# Predictions
y_train_pred_prin = xgb_model_prin.predict(X_train_prin)
y_val_pred_prin = xgb_model_prin.predict(X_val_prin)
y_test_pred_prin = xgb_model_prin.predict(X_test_prin)

# Evaluate model performance
train_mse_prin = mean_squared_error(y_train_prin, y_train_pred_prin)
val_mse_prin = mean_squared_error(y_val_prin, y_val_pred_prin)
test_mse_prin = mean_squared_error(y_test_prin, y_test_pred_prin)

train_r2_prin = r2_score(y_train_prin, y_train_pred_prin)
val_r2_prin = r2_score(y_val_prin, y_val_pred_prin)
test_r2_prin = r2_score(y_test_prin, y_test_pred_prin)

# Print results
print(f"Train MSE: {train_mse_prin:.4f}, R²: {train_r2_prin:.4f}")
print(f"Validation MSE: {val_mse_prin:.4f}, R²: {val_r2_prin:.4f}")
print(f"Test MSE: {test_mse_prin:.4f}, R²: {test_r2_prin:.4f}")



#plot actural vs predicetd reslts
plt.figure(figsize=(6, 6))
sns.scatterplot(x=y_test_prin, y=y_test_pred_prin)
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("XGBoost: Actual vs Predicted (Test Set)")
plt.plot([y_test_prin.min(), y_test_prin.max()],
         [y_test_prin.min(), y_test_prin.max()],
         'r--')
plt.show()









# Subsample 2% of the training data
X_train_sub_prin, _, y_train_sub_prin, _ = train_test_split(X_train_prin, y_train_prin, train_size=0.02, random_state=42, shuffle=True)

# Standardize the features
scaler_prin = StandardScaler()
X_train_sub_scaled_prin = scaler_prin.fit_transform(X_train_sub_prin)
X_val_scaled_prin = scaler_prin.transform(X_val_prin)
X_test_scaled_prin = scaler_prin.transform(X_test_prin)

# Train Linear SVM model (classification — if continuous target, use SVR instead)
svm_model_prin = LinearSVC(max_iter=1000, random_state=42)
svm_model_prin.fit(X_train_sub_scaled_prin, y_train_sub_prin)

# Predictions
y_train_pred_prin = svm_model_prin.predict(X_train_sub_scaled_prin)
y_val_pred_prin = svm_model_prin.predict(X_val_scaled_prin)
y_test_pred_prin = svm_model_prin.predict(X_test_scaled_prin)

# Evaluate model performance
train_mse_prin = mean_squared_error(y_train_sub_prin, y_train_pred_prin)
val_mse_prin = mean_squared_error(y_val_prin, y_val_pred_prin)
test_mse_prin = mean_squared_error(y_test_prin, y_test_pred_prin)

train_r2_prin = r2_score(y_train_sub_prin, y_train_pred_prin)
val_r2_prin = r2_score(y_val_prin, y_val_pred_prin)
test_r2_prin = r2_score(y_test_prin, y_test_pred_prin)

# Print results
print(f"Train MSE: {train_mse_prin:.4f}, R²: {train_r2_prin:.4f}")
print(f"Validation MSE: {val_mse_prin:.4f}, R²: {val_r2_prin:.4f}")
print(f"Test MSE: {test_mse_prin:.4f}, R²: {test_r2_prin:.4f}")



#plot the results 
plt.figure(figsize=(6, 6))
sns.scatterplot(x=y_test_prin, y=y_test_pred_prin)
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Linear SVM: Actual vs Predicted (Test Set)")
plt.plot([y_test_prin.min(), y_test_prin.max()],
         [y_test_prin.min(), y_test_prin.max()],
         'r--')
plt.show()






# Create a dictionary of results
results = {
    'Model': ['Linear Regression', 'Random Forest', 'XGBoost', 'Linear SVM (10%)'],
    'Train MSE': [4222554684.93, 13122.5781, 238522649.4237, 10889396403.4802],
    'Train R²': [0.2544, 0.9696, 0.9579, -0.7378],
    'Val MSE': [3914276643.5948, 35667.8978, 1185831992.6504, 11950641000.6477],
    'Val R²': [0.2850, 0.7676, 0.7834, -1.0756],
    'Test MSE': [4280889619.9658, 32811.5325, 1090246310.4710, 11890937938.6569],
    'Test R²': [0.2639, 0.8149, 0.8125, -1.1637]
}

# Convert to DataFrame
results_df = pd.DataFrame(results)

# Display it
results_df












# Initialize the SHAP explainer
explainer_rf = shap.TreeExplainer(rf_model_prin)

# Calculate the smaller sample size (cap it at 1000 rows)
sample_size = min(1000, X_train_prin.shape[0])

# Randomly select indices for the smaller sample size
sample_indices = np.random.choice(X_train_prin.shape[0], size=sample_size, replace=False)

# Subset the data by selected indices (using iloc for rows)
X_sample_prin = X_train_prin.iloc[sample_indices]

# Compute SHAP values for the smaller sample
shap_values_rf = explainer_rf.shap_values(X_sample_prin)

# Display the summary plot with actual feature names
shap.summary_plot(shap_values_rf, X_sample_prin)









#check correlation to avoid data leakage 
df[['CreateJob',"DisbursementGross", "Term", 'NoEmp', "NewExist", "UrbanRural_binary", 'RetainedJob', 'ChgOffPrinGr', 'SBA_Appv', 
            'ChgOffDate_binary', 'MIS_Status_Binary', 
            'IsFranchise', 'LowDoc_binary', 'RevLineCr_binary',
            'Sector_Accommodation & Food', 'Sector_Admin & Waste Mgmt',
            'Sector_Agriculture', 'Sector_Arts & Recreation', 'Sector_Construction',
            'Sector_Education', 'Sector_Finance', 'Sector_Healthcare',
            'Sector_Information', 'Sector_Management', 'Sector_Manufacturing',
            'Sector_Mining', 'Sector_Other Services', 'Sector_Professional Services',
            'Sector_Public Admin', 'Sector_Real Estate', 'Sector_Retail Trade',
            'Sector_Transportation', 'Sector_Unknown', 'Sector_Utilities',
            'Sector_Wholesale Trade']].corr()






# Take 100% sample of the dataset
#df = df.sample(frac=1, random_state=42)

# Define features and target variable
features = ["DisbursementGross", "Term", 'NoEmp', "NewExist", "UrbanRural_binary", 'RetainedJob', 'ChgOffPrinGr', 'SBA_Appv', 
            'ChgOffDate_binary', 'MIS_Status_Binary', 
            'IsFranchise', 'LowDoc_binary', 'RevLineCr_binary',
            'Sector_Accommodation & Food', 'Sector_Admin & Waste Mgmt',
            'Sector_Agriculture', 'Sector_Arts & Recreation', 'Sector_Construction',
            'Sector_Education', 'Sector_Finance', 'Sector_Healthcare',
            'Sector_Information', 'Sector_Management', 'Sector_Manufacturing',
            'Sector_Mining', 'Sector_Other Services', 'Sector_Professional Services',
            'Sector_Public Admin', 'Sector_Real Estate', 'Sector_Retail Trade',
            'Sector_Transportation', 'Sector_Unknown', 'Sector_Utilities',
            'Sector_Wholesale Trade']
X = df[features]
y = df['CreateJob']

# Split data into 70% train, 10% validation, 20% test
X_train_val_job, X_test_job, y_train_val_job, y_test_job = train_test_split(X, y, test_size=0.20, random_state=42)
X_train_job, X_val_job, y_train_job, y_val_job = train_test_split(X_train_val_job, y_train_val_job, test_size=0.125, random_state=42)



#see the split
print(f"Train set size: {X_train_job.shape[0]}")
print(f"Validation set size: {X_val_job.shape[0]}")
print(f"Test set size: {X_test_job.shape[0]}")






# Train Linear Regression model
lin_reg_job = LinearRegression()
lin_reg_job.fit(X_train_job, y_train_job)

# Predictions
y_train_pred_lin_job = lin_reg_job.predict(X_train_job)
y_val_pred_lin_job = lin_reg_job.predict(X_val_job)
y_test_pred_lin_job = lin_reg_job.predict(X_test_job)

# Evaluate model performance
train_mse_lin_job = mean_squared_error(y_train_job, y_train_pred_lin_job)
val_mse_lin_job = mean_squared_error(y_val_job, y_val_pred_lin_job)
test_mse_lin_job = mean_squared_error(y_test_job, y_test_pred_lin_job)

train_r2_lin_job = r2_score(y_train_job, y_train_pred_lin_job)
val_r2_lin_job = r2_score(y_val_job, y_val_pred_lin_job)
test_r2_lin_job = r2_score(y_test_job, y_test_pred_lin_job)

# Print results
print(f"Linear Regression:")
print(f"Train MSE: {train_mse_lin_job:.4f}, R²: {train_r2_lin_job:.4f}")
print(f"Validation MSE: {val_mse_lin_job:.4f}, R²: {val_r2_lin_job:.4f}")
print(f"Test MSE: {test_mse_lin_job:.4f}, R²: {test_r2_lin_job:.4f}")

# Plot Actual vs Predicted for test data
plt.figure(figsize=(6,6))
sns.scatterplot(x=y_test_job, y=y_test_pred_lin_job)
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title("Linear Regression: Actual vs Predicted (Test)")
plt.plot([y_test_job.min(), y_test_job.max()], [y_test_job.min(), y_test_job.max()], 'r--')  # 45-degree line
plt.show()






# Train Random Forest model
rf_model_job = RandomForestRegressor(n_estimators=10, random_state=42)
rf_model_job.fit(X_train_job, y_train_job)

# Predictions
y_train_pred_rf_job = rf_model_job.predict(X_train_job)
y_val_pred_rf_job = rf_model_job.predict(X_val_job)
y_test_pred_rf_job = rf_model_job.predict(X_test_job)

# Evaluate model performance
train_mse_rf_job = mean_squared_error(y_train_job, y_train_pred_rf_job)
val_mse_rf_job = mean_squared_error(y_val_job, y_val_pred_rf_job)
test_mse_rf_job = mean_squared_error(y_test_job, y_test_pred_rf_job)

train_r2_rf_job = r2_score(y_train_job, y_train_pred_rf_job)
val_r2_rf_job = r2_score(y_val_job, y_val_pred_rf_job)
test_r2_rf_job = r2_score(y_test_job, y_test_pred_rf_job)

# Print results
print(f"Random Forest:")
print(f"Train MSE: {train_mse_rf_job:.4f}, R²: {train_r2_rf_job:.4f}")
print(f"Validation MSE: {val_mse_rf_job:.4f}, R²: {val_r2_rf_job:.4f}")
print(f"Test MSE: {test_mse_rf_job:.4f}, R²: {test_r2_rf_job:.4f}")



# Plot Actual vs Predicted for Random Forest test data
plt.figure(figsize=(6, 6))
sns.scatterplot(x=y_test_job, y=y_test_pred_rf_job)
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title("Random Forest: Actual vs Predicted (Test)")
plt.plot([y_test_job.min(), y_test_job.max()], [y_test_job.min(), y_test_job.max()], 'r--')  # 45-degree line
plt.show()






# Convert target variable to integer type (if it's categorical)
y_train_job = y_train_job.astype(int)
y_test_job = y_test_job.astype(int)

# Check unique classes in train and test sets
print(np.unique(y_train_job))
print(np.unique(y_test_job))


# Train XGBoost model (regression)
xgb_model = XGBRegressor(n_estimators=50, random_state=42)
xgb_model.fit(X_train_job, y_train_job)

# Predictions
y_train_pred_xgb_job = xgb_model.predict(X_train_job)
y_val_pred_xgb_job = xgb_model.predict(X_val_job)
y_test_pred_xgb_job = xgb_model.predict(X_test_job)

# Evaluate model performance
train_mse = mean_squared_error(y_train_job, y_train_pred_xgb_job)
val_mse = mean_squared_error(y_val_job, y_val_pred_xgb_job)
test_mse = mean_squared_error(y_test_job, y_test_pred_xgb_job)

train_r2 = r2_score(y_train_job, y_train_pred_xgb_job)
val_r2 = r2_score(y_val_job, y_val_pred_xgb_job)
test_r2 = r2_score(y_test_job, y_test_pred_xgb_job)

# Print results
print(f"Train MSE: {train_mse:.4f}, R²: {train_r2:.4f}")
print(f"Validation MSE: {val_mse:.4f}, R²: {val_r2:.4f}")
print(f"Test MSE: {test_mse:.4f}, R²: {test_r2:.4f}")

# Plot Actual vs Predicted for XGBoost test data
plt.figure(figsize=(6, 6))
sns.scatterplot(x=y_test_job, y=y_test_pred_xgb_job)
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title("XGBoost: Actual vs Predicted (Test)")
plt.plot([y_test_job.min(), y_test_job.max()], [y_test_job.min(), y_test_job.max()], 'r--')  # 45-degree line
plt.show()



df.shape 





# Subsample 2% of the training data
X_train_sub2_job, _, y_train_sub2_job, _ = train_test_split(X_train_job, y_train_job, train_size=0.02, random_state=42, shuffle=True)

# Standardize the features
scaler_job = StandardScaler()
X_train_sub2_scaled_job = scaler_job.fit_transform(X_train_sub2_job)
X_val_scaled_job = scaler_job.transform(X_val_job)
X_test_scaled_job = scaler_job.transform(X_test_job)

# Train Linear SVM model
svm_model_job = LinearSVC(max_iter=1000, random_state=42)
svm_model_job.fit(X_train_sub2_scaled_job, y_train_sub2_job)

# Predictions
y_train_pred_svm_job = svm_model_job.predict(X_train_sub2_scaled_job)
y_val_pred_svm_job = svm_model_job.predict(X_val_scaled_job)
y_test_pred_svm_job = svm_model_job.predict(X_test_scaled_job)

# Evaluate model performance
train_mse_svm_job = mean_squared_error(y_train_sub2_job, y_train_pred_svm_job)
val_mse_svm_job = mean_squared_error(y_val_job, y_val_pred_svm_job)
test_mse_svm_job = mean_squared_error(y_test_job, y_test_pred_svm_job)

train_r2_svm_job = r2_score(y_train_sub2_job, y_train_pred_svm_job)
val_r2_svm_job = r2_score(y_val_job, y_val_pred_svm_job)
test_r2_svm_job = r2_score(y_test_job, y_test_pred_svm_job)

# Print results
print(f"Linear SVM:")
print(f"Train MSE: {train_mse_svm_job:.4f}, R²: {train_r2_svm_job:.4f}")
print(f"Validation MSE: {val_mse_svm_job:.4f}, R²: {val_r2_svm_job:.4f}")
print(f"Test MSE: {test_mse_svm_job:.4f}, R²: {test_r2_svm_job:.4f}")


# Plot Actual vs Predicted for SVM test data
plt.figure(figsize=(6, 6))
sns.scatterplot(x=y_test_job, y=y_test_pred_svm_job)
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title("SVM: Actual vs Predicted (Test)")
plt.plot([y_test_job.min(), y_test_job.max()], [y_test_job.min(), y_test_job.max()], 'r--')  # 45-degree line
plt.show()






# Create a dictionary of results
results_jobs = {
    'Model': ['Linear Regression', 'Random Forest', 'XGBoost', 'Linear SVM (10%)'],
    'Train MSE': [143.6146, 26.2989, 87.5700, 10889396403.4802],
    'Train R²': [0.0519, 0.8264, 0.4219, 0.0350],
    'Val MSE': [102.7379, 105.9432, 85.3770, 11950641000.6477],
    'Val R²': [-0.0590, -0.0921, 0.1199, -0.1586],
    'Test MSE': [360.3609, 386.1715, 369.3773, 11890937938.6569],
    'Test R²': [0.0265, -0.0432, 0.0022, -0.0101]
}

# Convert to DataFrame
results_df_jobs = pd.DataFrame(results_jobs)

# Display it
results_df_jobs







# Histogram of CreateJob
plt.figure(figsize=(8,5))
sns.histplot(df['CreateJob'], bins=30, kde=True)
plt.title('Distribution of Jobs Created')
plt.xlabel('Number of Jobs Created')
plt.ylabel('Frequency')
plt.show()


# Mean
print("Mean jobs created:", df['CreateJob'].mean())

# Max
print("Max jobs created:", df['CreateJob'].max())

# Min
print("Min jobs created:", df['CreateJob'].min())

# Standard deviation
print("Standard deviation:", df['CreateJob'].std())












#check correlation to avoid data leakage 
df[['MIS_Status_Binary', 'ChgOffPrinGr', 'ChgOffDate_binary', 'GrAppv']].corr()





# Define independent variables (features)
features = ['Term', 'NoEmp', 'NewExist', 'CreateJob', 'RetainedJob', 'DisbursementGross', 'SBA_Appv', 'GrAppv', 'UrbanRural_binary', 
            'IsFranchise', 'LowDoc_binary', 'RevLineCr_binary',
            'Sector_Accommodation & Food', 'Sector_Admin & Waste Mgmt',
            'Sector_Agriculture', 'Sector_Arts & Recreation', 'Sector_Construction',
            'Sector_Education', 'Sector_Finance', 'Sector_Healthcare',
            'Sector_Information', 'Sector_Management', 'Sector_Manufacturing',
            'Sector_Mining', 'Sector_Other Services', 'Sector_Professional Services',
            'Sector_Public Admin', 'Sector_Real Estate', 'Sector_Retail Trade',
            'Sector_Transportation', 'Sector_Unknown', 'Sector_Utilities',
            'Sector_Wholesale Trade']

# Define target variable
X = df[features]
y = df['MIS_Status_Binary']

# Split data into 70% train, 10% validation, 20% test
#first get 20% to be test data 
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.20, random_state=42)
#then split the train data further to have validation set 
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, random_state=42)


#see the split
print(f"Train set size: {X_train_val.shape[0]}")
print(f"Validation set size: {X_val.shape[0]}")
print(f"Test set size: {X_test.shape[0]}")





# Standardize the features (Logistic Regression performs better with normalized data)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)    # scale X_train
X_val_scaled = scaler.transform(X_val)            # scale X_val
X_test_scaled = scaler.transform(X_test)          # scale X_test

# Train Logistic Regression model
log_model = LogisticRegression()
log_model.fit(X_train_scaled, y_train)            # fit on X_train_scaled, y_train

# Predictions
y_train_pred = log_model.predict(X_train_scaled)
y_val_pred = log_model.predict(X_val_scaled)
y_test_pred = log_model.predict(X_test_scaled)

# Evaluate model performance
train_acc = accuracy_score(y_train, y_train_pred)
val_acc = accuracy_score(y_val, y_val_pred)
test_acc = accuracy_score(y_test, y_test_pred)

train_precision = precision_score(y_train, y_train_pred)
val_precision = precision_score(y_val, y_val_pred)
test_precision = precision_score(y_test, y_test_pred)

train_recall = recall_score(y_train, y_train_pred)
val_recall = recall_score(y_val, y_val_pred)
test_recall = recall_score(y_test, y_test_pred)

train_f1 = f1_score(y_train, y_train_pred)
val_f1 = f1_score(y_val, y_val_pred)
test_f1 = f1_score(y_test, y_test_pred)

# Print results
print(f"Train Accuracy: {train_acc:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}")
print(f"Validation Accuracy: {val_acc:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}")
print(f"Test Accuracy: {test_acc:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}")






# Compute confusion matrix
cm = confusion_matrix(y_test, y_test_pred)

# Plot confusion matrix
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - Test Set")
plt.show()





# Train Random Forest model with a different variable name
rf_model_MIS = RandomForestClassifier(n_estimators=10, random_state=42)
rf_model_MIS.fit(X_train, y_train)  # Use unscaled X_train
# Predictions
y_train_pred = rf_model_MIS.predict(X_train)
y_val_pred = rf_model_MIS.predict(X_val)
y_test_pred = rf_model_MIS.predict(X_test)

# Evaluate model performance
train_acc = accuracy_score(y_train, y_train_pred)
val_acc = accuracy_score(y_val, y_val_pred)
test_acc = accuracy_score(y_test, y_test_pred)

train_precision = precision_score(y_train, y_train_pred)
val_precision = precision_score(y_val, y_val_pred)
test_precision = precision_score(y_test, y_test_pred)

train_recall = recall_score(y_train, y_train_pred)
val_recall = recall_score(y_val, y_val_pred)
test_recall = recall_score(y_test, y_test_pred)

train_f1 = f1_score(y_train, y_train_pred)
val_f1 = f1_score(y_val, y_val_pred)
test_f1 = f1_score(y_test, y_test_pred)

# Print results
print(f"Train Accuracy: {train_acc:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}")
print(f"Validation Accuracy: {val_acc:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}")
print(f"Test Accuracy: {test_acc:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}")



# Compute confusion matrix
cm = confusion_matrix(y_test, y_test_pred)

# Plot confusion matrix
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Greens", cbar=False)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - Random Forest (Test Set)")
plt.show()





# Train XGBoost model
xgb_model = XGBClassifier(n_estimators=50, random_state=42, eval_metric="logloss")
xgb_model.fit(X_train, y_train)

# Predictions
y_train_pred = xgb_model.predict(X_train)
y_val_pred = xgb_model.predict(X_val)
y_test_pred = xgb_model.predict(X_test)

# Evaluate model performance
train_acc = accuracy_score(y_train, y_train_pred)
val_acc = accuracy_score(y_val, y_val_pred)
test_acc = accuracy_score(y_test, y_test_pred)

train_precision = precision_score(y_train, y_train_pred)
val_precision = precision_score(y_val, y_val_pred)
test_precision = precision_score(y_test, y_test_pred)

train_recall = recall_score(y_train, y_train_pred)
val_recall = recall_score(y_val, y_val_pred)
test_recall = recall_score(y_test, y_test_pred)

train_f1 = f1_score(y_train, y_train_pred)
val_f1 = f1_score(y_val, y_val_pred)
test_f1 = f1_score(y_test, y_test_pred)

# Print results
print(f"Train Accuracy: {train_acc:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}")
print(f"Validation Accuracy: {val_acc:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}")
print(f"Test Accuracy: {test_acc:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}")



# Compute confusion matrix
cm = confusion_matrix(y_test, y_test_pred)

# Plot confusion matrix
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Purples", cbar=False)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - XGBoost (Test Set)")
plt.show()








# Subsample 2% of the training data
X_train_sub3, _, y_train_sub3, _ = train_test_split(X_train, y_train, train_size=0.02, random_state=42, shuffle=True)

# Scale features
scaler = StandardScaler()
X_train_sub3_scaled = scaler.fit_transform(X_train_sub3)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Train LinearSVC model with scaled data
svm_model = LinearSVC(max_iter=500, random_state=42)
svm_model.fit(X_train_sub3_scaled, y_train_sub3)

# Predictions on scaled data
y_train_pred = svm_model.predict(X_train_sub3_scaled)
y_val_pred = svm_model.predict(X_val_scaled)
y_test_pred = svm_model.predict(X_test_scaled)

# Evaluate model performance
train_mse = mean_squared_error(y_train_sub3, y_train_pred)
val_mse = mean_squared_error(y_val, y_val_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

train_r2 = r2_score(y_train_sub3, y_train_pred)
val_r2 = r2_score(y_val, y_val_pred)
test_r2 = r2_score(y_test, y_test_pred)

train_acc = accuracy_score(y_train_sub3, y_train_pred)
val_acc = accuracy_score(y_val, y_val_pred)
test_acc = accuracy_score(y_test, y_test_pred)

# Print results
print(f"Train MSE: {train_mse:.4f}, R²: {train_r2:.4f}, Accuracy: {train_acc:.4f}")
print(f"Validation MSE: {val_mse:.4f}, R²: {val_r2:.4f}, Accuracy: {val_acc:.4f}")
print(f"Test MSE: {test_mse:.4f}, R²: {test_r2:.4f}, Accuracy: {test_acc:.4f}")





# Compute confusion matrix
cm = confusion_matrix(y_test, y_test_pred)

# Plot confusion matrix
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="BuGn", cbar=False)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - LinearSVC (Test Set)")
plt.show()





# Compare model performance 
# Create a dictionary of repayment results 
results_repayment = {
    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost', 'Linear SVM'],
    'Train MSE': [0.0070, 0.0016, 0.0070, 0.1980],
    'Train R²': [0.9611, 0.9912, 0.9613, -0.1258],
    'Train Accuracy': [0.8035, 0.9936, 0.9303, 0.8020],
    'Val MSE': [0.0071, 0.0074, 0.0071, 0.2054],
    'Val R²': [0.9608, 0.9587, 0.9608, -0.1394],
    'Val Accuracy': [0.8042, 0.9055, 0.9284, 0.7946],
    'Test MSE': [0.0068, 0.0071, 0.0069, 0.2032],
    'Test R²': [0.9618, 0.9602, 0.9617, -0.1324],
    'Test Accuracy': [0.8039, 0.9073, 0.9293, 0.7968]
}

# Convert to DataFrame
results_df_repayment = pd.DataFrame(results_repayment)

# Display it
results_df_repayment













# Create SHAP explainer for the XGBoost model
explainer = shap.TreeExplainer(xgb_model)

# Compute SHAP values for the training set
shap_values = explainer.shap_values(X_train)

# Summary plot
shap.summary_plot(shap_values, features=X_train, feature_names=X_train.columns)

# To extract mean absolute SHAP values per feature
shap_df = pd.DataFrame({
    'feature': X_train.columns,
    'mean_abs_shap': np.abs(shap_values).mean(axis=0)
})

# Sort by importance
shap_df = shap_df.sort_values(by='mean_abs_shap', ascending=False)

# Display ranked feature importances
print(shap_df)









#check correlation to avoid data leakage 
df[['ChgOffDate_binary','MIS_Status_Binary', 'ChgOffPrinGr', 'DisbursementGross']].corr()






# Define independent variables (features)
features = ['Term', 'NoEmp', 'NewExist', 'CreateJob', 'RetainedJob', 'DisbursementGross', 'SBA_Appv', 'UrbanRural_binary', 
            'IsFranchise', 'LowDoc_binary', 'RevLineCr_binary',
            'Sector_Accommodation & Food', 'Sector_Admin & Waste Mgmt',
            'Sector_Agriculture', 'Sector_Arts & Recreation', 'Sector_Construction',
            'Sector_Education', 'Sector_Finance', 'Sector_Healthcare',
            'Sector_Information', 'Sector_Management', 'Sector_Manufacturing',
            'Sector_Mining', 'Sector_Other Services', 'Sector_Professional Services',
            'Sector_Public Admin', 'Sector_Real Estate', 'Sector_Retail Trade',
            'Sector_Transportation', 'Sector_Unknown', 'Sector_Utilities',
            'Sector_Wholesale Trade']

# Define target variable
X = df[features]
y = df['ChgOffDate_binary']

# Split data into 70% train, 10% validation, 20% test
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.20, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, random_state=42)


#see the split
print(f"Train set size: {X_train.shape[0]}")
print(f"Validation set size: {X_val.shape[0]}")
print(f"Test set size: {X_test.shape[0]}")





# Standardize the features (Logistic Regression performs better with normalized data)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Train Logistic Regression model
log_model = LogisticRegression(random_state=42)
log_model.fit(X_train_scaled, y_train)

# Predictions
y_train_pred = log_model.predict(X_train_scaled)
y_val_pred = log_model.predict(X_val_scaled)
y_test_pred = log_model.predict(X_test_scaled)

# Evaluate model performance
train_acc = accuracy_score(y_train, y_train_pred)
val_acc = accuracy_score(y_val, y_val_pred)
test_acc = accuracy_score(y_test, y_test_pred)

train_precision = precision_score(y_train, y_train_pred)
val_precision = precision_score(y_val, y_val_pred)
test_precision = precision_score(y_test, y_test_pred)

train_recall = recall_score(y_train, y_train_pred)
val_recall = recall_score(y_val, y_val_pred)
test_recall = recall_score(y_test, y_test_pred)

train_f1 = f1_score(y_train, y_train_pred)
val_f1 = f1_score(y_val, y_val_pred)
test_f1 = f1_score(y_test, y_test_pred)

# Print results
print(f"Train Accuracy: {train_acc:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}")
print(f"Validation Accuracy: {val_acc:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}")
print(f"Test Accuracy: {test_acc:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}")






# Compute confusion matrix
cm = confusion_matrix(y_test, y_test_pred)

# Plot confusion matrix heatmap
plt.figure(figsize=(5, 4))
sns.heatmap(cm, annot=True, fmt="d", cmap="YlGnBu", cbar=False)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - Logistic Regression (Test Set)")
plt.show()





# Train RandomForestClassifier model using the already split data
rf_model = RandomForestClassifier(n_estimators=10, random_state=42)
rf_model.fit(X_train, y_train)  # Use unscaled X_train

# Predictions
y_train_pred = rf_model.predict(X_train)
y_val_pred = rf_model.predict(X_val)
y_test_pred = rf_model.predict(X_test)

# Evaluate model performance
train_acc = accuracy_score(y_train, y_train_pred)
val_acc = accuracy_score(y_val, y_val_pred)
test_acc = accuracy_score(y_test, y_test_pred)
train_precision = precision_score(y_train, y_train_pred)
val_precision = precision_score(y_val, y_val_pred)
test_precision = precision_score(y_test, y_test_pred)
train_recall = recall_score(y_train, y_train_pred)
val_recall = recall_score(y_val, y_val_pred)
test_recall = recall_score(y_test, y_test_pred)
train_f1 = f1_score(y_train, y_train_pred)
val_f1 = f1_score(y_val, y_val_pred)
test_f1 = f1_score(y_test, y_test_pred)

# Print results
print(f"Train Accuracy: {train_acc:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}")
print(f"Validation Accuracy: {val_acc:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}")
print(f"Test Accuracy: {test_acc:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}")





# Compute confusion matrix
cm = confusion_matrix(y_test, y_test_pred)

# Plot confusion matrix heatmap
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="YlGnBu", cbar=False)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - Random Forest (Test Set)")
plt.show()





# Train XGBoost model (using already split data)
xgb_model = XGBClassifier(n_estimators=50, random_state=42, eval_metric="logloss")
xgb_model.fit(X_train, y_train)

# Predictions
y_train_pred = xgb_model.predict(X_train)
y_val_pred = xgb_model.predict(X_val)
y_test_pred = xgb_model.predict(X_test)

# Evaluate model performance
train_acc = accuracy_score(y_train, y_train_pred)
val_acc = accuracy_score(y_val, y_val_pred)
test_acc = accuracy_score(y_test, y_test_pred)

train_precision = precision_score(y_train, y_train_pred)
val_precision = precision_score(y_val, y_val_pred)
test_precision = precision_score(y_test, y_test_pred)

train_recall = recall_score(y_train, y_train_pred)
val_recall = recall_score(y_val, y_val_pred)
test_recall = recall_score(y_test, y_test_pred)

train_f1 = f1_score(y_train, y_train_pred)
val_f1 = f1_score(y_val, y_val_pred)
test_f1 = f1_score(y_test, y_test_pred)

# Print results
print(f"Train Accuracy: {train_acc:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}")
print(f"Validation Accuracy: {val_acc:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}")
print(f"Test Accuracy: {test_acc:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}")






# Compute confusion matrix
cm = confusion_matrix(y_test, y_test_pred)

# Plot confusion matrix heatmap
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="YlGnBu", cbar=False)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - XGBoost (Test Set)")
plt.show()





# Subsample 2% of the training data
X_train_sub4, _, y_train_sub4, _ = train_test_split(X_train, y_train, train_size=0.02, random_state=42, shuffle=True)

# Standardize features
scaler = StandardScaler()
X_train_sub4_scaled = scaler.fit_transform(X_train_sub4)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Train Linear SVM model
svm_model = LinearSVC(max_iter=1000, random_state=42)
svm_model.fit(X_train_sub4_scaled, y_train_sub4)

# Predictions
y_train_pred = svm_model.predict(X_train_sub4_scaled)
y_val_pred = svm_model.predict(X_val_scaled)
y_test_pred = svm_model.predict(X_test_scaled)

# Evaluate model performance
train_acc = accuracy_score(y_train_sub4, y_train_pred)
val_acc = accuracy_score(y_val, y_val_pred)
test_acc = accuracy_score(y_test, y_test_pred)

train_precision = precision_score(y_train_sub4, y_train_pred)
val_precision = precision_score(y_val, y_val_pred)
test_precision = precision_score(y_test, y_test_pred)

train_recall = recall_score(y_train_sub4, y_train_pred)
val_recall = recall_score(y_val, y_val_pred)
test_recall = recall_score(y_test, y_test_pred)

train_f1 = f1_score(y_train_sub4, y_train_pred)
val_f1 = f1_score(y_val, y_val_pred)
test_f1 = f1_score(y_test, y_test_pred)

# Print results
print(f"Train Accuracy: {train_acc:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}")
print(f"Validation Accuracy: {val_acc:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}")
print(f"Test Accuracy: {test_acc:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}")






# Compute confusion matrix
cm = confusion_matrix(y_test, y_test_pred)

# Plot confusion matrix
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="BuGn", cbar=False)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - LinearSVC (Test Set)")
plt.show()



#model performance comparison 
#dictionary with results of each model 
results_default = {
    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost', 'Linear SVM'],
    'Train MSE': [0.0070, 0.0016, 0.0070, 0.1975],
    'Train R²': [0.9611, 0.9912, 0.9613, -0.1230],
    'Train Accuracy': [0.8069, 0.9920, 0.9365, 0.8052],
    'Val MSE': [0.0071, 0.0074, 0.0071, 0.2051],
    'Val R²': [0.9608, 0.9587, 0.9608, -0.1378],
    'Val Accuracy': [0.8070, 0.9076, 0.9347, 0.7974],
    'Test MSE': [0.0068, 0.0071, 0.0069, 0.2028],
    'Test R²': [0.9618, 0.9602, 0.9617, -0.1303],
    'Test Accuracy': [0.8071, 0.9077, 0.9356, 0.7987]
}


# Convert to DataFrame
results_df_default = pd.DataFrame(results_default)

# Display results 
results_df_default








# SHAP explainer for the XGB model
explainer = shap.TreeExplainer(xgb_model)

# Compute SHAP values for the training set
shap_values = explainer(X_train)

# Summary plot
shap.summary_plot(shap_values, features=X_train, feature_names=X_train.columns)

# extract mean absolute SHAP values per feature
shap_df = pd.DataFrame({
    'feature': X_train.columns,
    'mean_abs_shap': np.abs(shap_values.values).mean(axis=0)
})

# Sort by importance
shap_df = shap_df.sort_values(by='mean_abs_shap', ascending=False)

# Display ranked feature importances
print(shap_df)



