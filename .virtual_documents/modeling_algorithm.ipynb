#import libraries 
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.ticker as mtick
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor



#read pre-processed data 
df = pd.read_csv(r"C:\Users\Ochieng' Oginga\Documents\Post_S\Spring_2025\Data_Science_II\Project\Data\Oginga_Stage3.csv")

#show head 
df.head()








#get description of the data 
df.describe(include='all')






#get summary of the numeric variables 
df.describe()





#summary of categorical variables 
df.describe(include=[object])








#create a histogram for bumeric variables with bins of 20 
df.hist(figsize=(12, 10), bins=20)





 # Ensure ApprovalFY is numeric
df['ApprovalFY'] = pd.to_numeric(df['ApprovalFY'], errors='coerce')

# Drop any NaN values that might have been introduced during conversion
df = df.dropna(subset=['ApprovalFY'])

# Aggregate and plot
df.groupby('ApprovalFY')['GrAppv'].sum().plot(kind='bar', figsize=(10, 5))

plt.xlabel("Approval Fiscal Year")
plt.ylabel("Approved Gross ($)")
plt.title("Total Approved Gross by ApprovalFY")
plt.xticks(rotation=45)
plt.show()








# Extract the last two digits (year) from DisbursementDate and convert to integer
df['DisbursementYear'] = df['DisbursementDate'].str[-2:].astype(int)

# Fix potential century issue: Convert 00-23 to 2000-2023, and 80-99 to 1980-1999
df['DisbursementYear'] = df['DisbursementYear'].apply(lambda x: x + 2000 if x < 24 else x + 1900)

# Aggregate total disbursement by year
yearly_disbursement = df.groupby('DisbursementYear')['DisbursementGross'].sum()

# Plot
fig, ax = plt.subplots(figsize=(10, 5))
yearly_disbursement.plot(kind='bar', ax=ax)

# Formatting
ax.set_xlabel("Disbursement Year")
ax.set_ylabel("Total Loan Disbursement ($)")
ax.set_title("Total Loan Disbursement by Year")
ax.yaxis.set_major_formatter(mtick.StrMethodFormatter('${x:,.0f}'))  # Format as dollars
plt.xticks(rotation=45)

plt.show()








#box plotting
plt.figure(figsize=(12, 6))
sns.boxplot(data=df.select_dtypes(include=['number']))
plt.xticks(rotation=90)
plt.show()









#check columns 
df.columns











#set features and target 
features = ['Term', 'NoEmp', 'NewExist', 'CreateJob', 'RetainedJob', 'DisbursementGross', 'ChgOffPrinGr', 'SBA_Appv', 
            'ChgOffDate_binary', 'UrbanRural_binary', 'MIS_Status_Binary', 
            'IsFranchise', 'LowDoc_binary', 'RevLineCr_binary',
            'Sector_Accommodation & Food', 'Sector_Admin & Waste Mgmt',
            'Sector_Agriculture', 'Sector_Arts & Recreation', 'Sector_Construction',
            'Sector_Education', 'Sector_Finance', 'Sector_Healthcare',
            'Sector_Information', 'Sector_Management', 'Sector_Manufacturing',
            'Sector_Mining', 'Sector_Other Services', 'Sector_Professional Services',
            'Sector_Public Admin', 'Sector_Real Estate', 'Sector_Retail Trade',
            'Sector_Transportation', 'Sector_Unknown', 'Sector_Utilities',
            'Sector_Wholesale Trade']

X = df[features]
y = df['GrAppv']

#Split into 80% train+val, 20% test
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

#Split train_val into 87.5% train, 12.5% val ( to achieve 70% train, 10% val)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, random_state=42)

#Scale features for standardization 
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Fit Linear Regression Model
lin_reg = LinearRegression()
lin_reg.fit(X_train_scaled, y_train)

# Predict
y_train_pred = lin_reg.predict(X_train_scaled)
y_val_pred = lin_reg.predict(X_val_scaled)
y_test_pred = lin_reg.predict(X_test_scaled)

# Evaluate
def evaluate(y_true, y_pred, label=""):
    mse = mean_squared_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    print(f"{label} MSE: {mse:.2f}, R²: {r2:.4f}")

evaluate(y_train, y_train_pred, "Train")
evaluate(y_val, y_val_pred, "Validation")
evaluate(y_test, y_test_pred, "Test")





# Train Random Forest Regressor with scaled data
rf_model = RandomForestRegressor(
    n_estimators=100,
    max_depth=10,
    n_jobs=-1,
    random_state=42
)
rf_model.fit(X_train_scaled, y_train)  # Use scaled features

# Predictions
y_train_pred_rf = rf_model.predict(X_train_scaled)
y_val_pred_rf = rf_model.predict(X_val_scaled)
y_test_pred_rf = rf_model.predict(X_test_scaled)

# Evaluate model performance
train_mse_rf = mean_squared_error(y_train, y_train_pred_rf)
val_mse_rf = mean_squared_error(y_val, y_val_pred_rf)
test_mse_rf = mean_squared_error(y_test, y_test_pred_rf)

train_r2_rf = r2_score(y_train, y_train_pred_rf)
val_r2_rf = r2_score(y_val, y_val_pred_rf)
test_r2_rf = r2_score(y_test, y_test_pred_rf)

# Print results
print(f"Random Forest Train MSE: {train_mse_rf:.4f}, R²: {train_r2_rf:.4f}")
print(f"Random Forest Validation MSE: {val_mse_rf:.4f}, R²: {val_r2_rf:.4f}")
print(f"Random Forest Test MSE: {test_mse_rf:.4f}, R²: {test_r2_rf:.4f}")







# Train XGBoost Regressor
xgb_model = XGBRegressor(n_estimators=50, random_state=42, eval_metric="rmse", n_jobs=-1)
xgb_model.fit(X_train, y_train)

# Predictions
y_train_pred = xgb_model.predict(X_train)
y_val_pred = xgb_model.predict(X_val)
y_test_pred = xgb_model.predict(X_test)

# Evaluate model performance
train_mse = mean_squared_error(y_train, y_train_pred)
val_mse = mean_squared_error(y_val, y_val_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

train_r2 = r2_score(y_train, y_train_pred)
val_r2 = r2_score(y_val, y_val_pred)
test_r2 = r2_score(y_test, y_test_pred)

# Print results
print(f"Train MSE: {train_mse:.4f}, R²: {train_r2:.4f}")
print(f"Validation MSE: {val_mse:.4f}, R²: {val_r2:.4f}")
print(f"Test MSE: {test_mse:.4f}, R²: {test_r2:.4f}")






from sklearn.svm import LinearSVR
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.pipeline import make_pipeline

# Create a pipeline with StandardScaler, PCA, and LinearSVR
svr_pipeline = make_pipeline(
    StandardScaler(),
    PCA(n_components=0.95),  # Retain 95% of variance
    LinearSVR(random_state=42, max_iter=10000)
)

# Fit model on training data
svr_pipeline.fit(X_train, y_train)

# Predictions
y_train_pred = svr_pipeline.predict(X_train)
y_val_pred = svr_pipeline.predict(X_val)
y_test_pred = svr_pipeline.predict(X_test)

# Evaluate model performance
train_mse = mean_squared_error(y_train, y_train_pred)
val_mse = mean_squared_error(y_val, y_val_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

train_r2 = r2_score(y_train, y_train_pred)
val_r2 = r2_score(y_val, y_val_pred)
test_r2 = r2_score(y_test, y_test_pred)

# Print results
print(f"Train MSE: {train_mse:.4f}, R²: {train_r2:.4f}")
print(f"Validation MSE: {val_mse:.4f}, R²: {val_r2:.4f}")
print(f"Test MSE: {test_mse:.4f}, R²: {test_r2:.4f}")



import matplotlib.pyplot as plt
import pandas as pd

# Predictions for each model
y_test_pred_lin = lin_reg.predict(X_test_scaled)          # Linear Regression (trained on scaled)
y_test_pred_rf = rf_model.predict(X_test_scaled)          # Random Forest (trained on scaled)
y_test_pred_xgb = xgb_model.predict(X_test)               # XGBoost (trained on unscaled)
y_test_pred_svr = svr_pipeline.predict(X_test)            # SVM pipeline handles scaling

# Store predictions in a DataFrame
df_plot_multi = pd.DataFrame({
    'Actual': y_test.values,
    'LinearReg': y_test_pred_lin,
    'RandomForest': y_test_pred_rf,
    'XGBoost': y_test_pred_xgb,
    'SVM': y_test_pred_svr
})

# Sort and bin by actual values
df_plot_multi = df_plot_multi.sort_values(by='Actual').reset_index(drop=True)
df_plot_multi['bin'] = df_plot_multi.index // 100  # 100 observations per bin

# Group by bin and compute mean
df_binned_multi = df_plot_multi.groupby('bin').mean()

# Plot
plt.figure(figsize=(10, 6))
plt.plot(df_binned_multi['Actual'], df_binned_multi['LinearReg'], label='Linear Regression', linestyle='-', marker='o')
plt.plot(df_binned_multi['Actual'], df_binned_multi['RandomForest'], label='Random Forest', linestyle='-', marker='x')
plt.plot(df_binned_multi['Actual'], df_binned_multi['XGBoost'], label='XGBoost', linestyle='-', marker='^')
plt.plot(df_binned_multi['Actual'], df_binned_multi['SVM'], label='SVM', linestyle='-', marker='s')
plt.plot(df_binned_multi['Actual'], df_binned_multi['Actual'], linestyle='--', color='black', label='Perfect Prediction')

# Labels and title
plt.xlabel('Average Actual Gross Approval ($)')
plt.ylabel('Average Predicted Gross Approval ($)')
plt.title('Comparison: Binned Predicted vs Actual for Multiple Models')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()









# Model Evaluation
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    ConfusionMatrixDisplay,
    mean_squared_error,
    r2_score
)

# Models
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier, XGBRegressor

# Preprocessing and Model Selection
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler







# Define features and target variable
features = ['NoEmp', 'NewExist', 'DisbursementGross', 'GrAppv', 'SBA_Appv', 'UrbanRural_binary','MIS_Status_Binary', 
            'IsFranchise', 'LowDoc_binary', 'RevLineCr_binary',
            'Sector_Accommodation & Food', 'Sector_Admin & Waste Mgmt',
            'Sector_Agriculture', 'Sector_Arts & Recreation', 'Sector_Construction',
            'Sector_Education', 'Sector_Finance', 'Sector_Healthcare',
            'Sector_Information', 'Sector_Management', 'Sector_Manufacturing',
            'Sector_Mining', 'Sector_Other Services', 'Sector_Professional Services',
            'Sector_Public Admin', 'Sector_Real Estate', 'Sector_Retail Trade',
            'Sector_Transportation', 'Sector_Unknown', 'Sector_Utilities',
            'Sector_Wholesale Trade']

X = df[features]
y = df['ChgOffDate_binary']

# Split data into 70% train, 10% validation, 20% test
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.20, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, random_state=42)

# Train Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Predictions
y_train_pred = rf_model.predict(X_train)
y_val_pred = rf_model.predict(X_val)
y_test_pred = rf_model.predict(X_test)

# Evaluate model performance
train_acc = accuracy_score(y_train, y_train_pred)
val_acc = accuracy_score(y_val, y_val_pred)
test_acc = accuracy_score(y_test, y_test_pred)

train_precision = precision_score(y_train, y_train_pred)
val_precision = precision_score(y_val, y_val_pred)
test_precision = precision_score(y_test, y_test_pred)

train_recall = recall_score(y_train, y_train_pred)
val_recall = recall_score(y_val, y_val_pred)
test_recall = recall_score(y_test, y_test_pred)

train_f1 = f1_score(y_train, y_train_pred)
val_f1 = f1_score(y_val, y_val_pred)
test_f1 = f1_score(y_test, y_test_pred)

# Print results
print(f"Train Accuracy: {train_acc:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}")
print(f"Validation Accuracy: {val_acc:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}")
print(f"Test Accuracy: {test_acc:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}")

# Confusion Matrix
print("\nTest Confusion Matrix:")
cm = confusion_matrix(y_test, y_test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()





# Train XGBoost model (regression)
xgb_model = XGBRegressor(n_estimators=50, random_state=42)
xgb_model.fit(X_train, y_train)

# Predictions
y_train_pred = xgb_model.predict(X_train)
y_val_pred = xgb_model.predict(X_val)
y_test_pred = xgb_model.predict(X_test)

# Evaluate model performance
train_mse = mean_squared_error(y_train, y_train_pred)
val_mse = mean_squared_error(y_val, y_val_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

train_r2 = r2_score(y_train, y_train_pred)
val_r2 = r2_score(y_val, y_val_pred)
test_r2 = r2_score(y_test, y_test_pred)

# Print results
print(f"Train MSE: {train_mse:.4f}, R²: {train_r2:.4f}")
print(f"Validation MSE: {val_mse:.4f}, R²: {val_r2:.4f}")
print(f"Test MSE: {test_mse:.4f}, R²: {test_r2:.4f}")









# Define features and target variable
features = ["DisbursementGross", "Term",'NoEmp', "NewExist", "UrbanRural_binary", 'RetainedJob', 'ChgOffPrinGr', 'SBA_Appv', 
            'ChgOffDate_binary', 'MIS_Status_Binary', 
            'IsFranchise', 'LowDoc_binary', 'RevLineCr_binary',
            'Sector_Accommodation & Food', 'Sector_Admin & Waste Mgmt',
            'Sector_Agriculture', 'Sector_Arts & Recreation', 'Sector_Construction',
            'Sector_Education', 'Sector_Finance', 'Sector_Healthcare',
            'Sector_Information', 'Sector_Management', 'Sector_Manufacturing',
            'Sector_Mining', 'Sector_Other Services', 'Sector_Professional Services',
            'Sector_Public Admin', 'Sector_Real Estate', 'Sector_Retail Trade',
            'Sector_Transportation', 'Sector_Unknown', 'Sector_Utilities',
            'Sector_Wholesale Trade']

X = df[features]
y = df['CreateJob']  # Target variable changed to 'CreateJob'

# Split data into 70% train, 10% validation, 20% test
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.20, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, random_state=42)


# Train Random Forest model
rf_model = RandomForestClassifier(n_estimators=10, random_state=42)
rf_model.fit(X_train, y_train)

# Predictions
y_train_pred = rf_model.predict(X_train)
y_val_pred = rf_model.predict(X_val)
y_test_pred = rf_model.predict(X_test)

# Evaluate model performance
train_acc = accuracy_score(y_train, y_train_pred)
val_acc = accuracy_score(y_val, y_val_pred)
test_acc = accuracy_score(y_test, y_test_pred)

# Add zero_division to avoid warnings
train_precision = precision_score(y_train, y_train_pred, average='macro', zero_division=1)
val_precision = precision_score(y_val, y_val_pred, average='macro', zero_division=1)
test_precision = precision_score(y_test, y_test_pred, average='macro', zero_division=1)

train_recall = recall_score(y_train, y_train_pred, average='macro', zero_division=1)
val_recall = recall_score(y_val, y_val_pred, average='macro', zero_division=1)
test_recall = recall_score(y_test, y_test_pred, average='macro', zero_division=1)

train_f1 = f1_score(y_train, y_train_pred, average='macro')
val_f1 = f1_score(y_val, y_val_pred, average='macro')
test_f1 = f1_score(y_test, y_test_pred, average='macro')


# Print results
print(f"Train Accuracy: {train_acc:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}")
print(f"Validation Accuracy: {val_acc:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}")
print(f"Test Accuracy: {test_acc:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}")






# Convert target variable to integer type (if it's categorical)
y_train = y_train.astype(int)
y_test = y_test.astype(int)

# Check unique classes in train and test sets
print(np.unique(y_train))
print(np.unique(y_test))


#import library 
from xgboost import XGBRegressor
# Train XGBoost model (regression)
xgb_model = XGBRegressor(n_estimators=50, random_state=42)
xgb_model.fit(X_train, y_train)

# Predictions
y_train_pred = xgb_model.predict(X_train)
y_val_pred = xgb_model.predict(X_val)
y_test_pred = xgb_model.predict(X_test)

# Evaluate model performance
train_mse = mean_squared_error(y_train, y_train_pred)
val_mse = mean_squared_error(y_val, y_val_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

train_r2 = r2_score(y_train, y_train_pred)
val_r2 = r2_score(y_val, y_val_pred)
test_r2 = r2_score(y_test, y_test_pred)

# Print results
print(f"Train MSE: {train_mse:.4f}, R^2: {train_r2:.4f}")
print(f"Validation MSE: {val_mse:.4f}, R^2: {val_r2:.4f}")
print(f"Test MSE: {test_mse:.4f}, R^2: {test_r2:.4f}")















# Define independent variables (features)
features = ['Term', 'NoEmp', 'NewExist', 'CreateJob', 'RetainedJob', 'DisbursementGross', 'ChgOffPrinGr', 'SBA_Appv', 
            'ChgOffDate_binary', 'UrbanRural_binary', 
            'IsFranchise', 'LowDoc_binary', 'RevLineCr_binary',
            'Sector_Accommodation & Food', 'Sector_Admin & Waste Mgmt',
            'Sector_Agriculture', 'Sector_Arts & Recreation', 'Sector_Construction',
            'Sector_Education', 'Sector_Finance', 'Sector_Healthcare',
            'Sector_Information', 'Sector_Management', 'Sector_Manufacturing',
            'Sector_Mining', 'Sector_Other Services', 'Sector_Professional Services',
            'Sector_Public Admin', 'Sector_Real Estate', 'Sector_Retail Trade',
            'Sector_Transportation', 'Sector_Unknown', 'Sector_Utilities',
            'Sector_Wholesale Trade']

# Define target variable
X = df[features]
y = df['MIS_Status_Binary']

# Split data into 70% train, 10% validation, 20% test
#first get 20% to be test data 
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.20, random_state=42)
#then split the train data further to have validation set 
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, random_state=42)

# Standardize the features (Logistic Regression performs better with normalized data)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Train Logistic Regression model
log_model = LogisticRegression()
log_model.fit(X_train_scaled, y_train)

# Predictions
y_train_pred = log_model.predict(X_train_scaled)
y_val_pred = log_model.predict(X_val_scaled)
y_test_pred = log_model.predict(X_test_scaled)

# Evaluate model performance
train_mse = mean_squared_error(y_train, y_train_pred)
val_mse = mean_squared_error(y_val, y_val_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

train_r2 = r2_score(y_train, y_train_pred)
val_r2 = r2_score(y_val, y_val_pred)
test_r2 = r2_score(y_test, y_test_pred)

train_acc = accuracy_score(y_train, y_train_pred)
val_acc = accuracy_score(y_val, y_val_pred)
test_acc = accuracy_score(y_test, y_test_pred)

# Print results
print(f"Train MSE: {train_mse:.4f}, R²: {train_r2:.4f}, Accuracy: {train_acc:.4f}")
print(f"Validation MSE: {val_mse:.4f}, R²: {val_r2:.4f}, Accuracy: {val_acc:.4f}")
print(f"Test MSE: {test_mse:.4f}, R²: {test_r2:.4f}, Accuracy: {test_acc:.4f}")






# Train Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)  # Use unscaled X_train

# Predictions
y_train_pred = rf_model.predict(X_train)
y_val_pred = rf_model.predict(X_val)
y_test_pred = rf_model.predict(X_test)

# Evaluate model performance
train_mse = mean_squared_error(y_train, y_train_pred)
val_mse = mean_squared_error(y_val, y_val_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

train_r2 = r2_score(y_train, y_train_pred)
val_r2 = r2_score(y_val, y_val_pred)
test_r2 = r2_score(y_test, y_test_pred)

train_acc = accuracy_score(y_train, y_train_pred)
val_acc = accuracy_score(y_val, y_val_pred)
test_acc = accuracy_score(y_test, y_test_pred)

# Print results
print(f"Train MSE: {train_mse:.4f}, R²: {train_r2:.4f}, Accuracy: {train_acc:.4f}")
print(f"Validation MSE: {val_mse:.4f}, R²: {val_r2:.4f}, Accuracy: {val_acc:.4f}")
print(f"Test MSE: {test_mse:.4f}, R²: {test_r2:.4f}, Accuracy: {test_acc:.4f}")






# Train XGBoost model
xgb_model = XGBClassifier(n_estimators=50, random_state=42, eval_metric="logloss")
xgb_model.fit(X_train, y_train)

# Predictions
y_train_pred = xgb_model.predict(X_train)
y_val_pred = xgb_model.predict(X_val)
y_test_pred = xgb_model.predict(X_test)

# Evaluate model performance
train_mse = mean_squared_error(y_train, y_train_pred)
val_mse = mean_squared_error(y_val, y_val_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

train_r2 = r2_score(y_train, y_train_pred)
val_r2 = r2_score(y_val, y_val_pred)
test_r2 = r2_score(y_test, y_test_pred)

train_acc = accuracy_score(y_train, y_train_pred)
val_acc = accuracy_score(y_val, y_val_pred)
test_acc = accuracy_score(y_test, y_test_pred)

# Print results
print(f"Train MSE: {train_mse:.4f}, R²: {train_r2:.4f}, Accuracy: {train_acc:.4f}")
print(f"Validation MSE: {val_mse:.4f}, R²: {val_r2:.4f}, Accuracy: {val_acc:.4f}")
print(f"Test MSE: {test_mse:.4f}, R²: {test_r2:.4f}, Accuracy: {test_acc:.4f}")






#scale features 
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train SVM model with scaled data
svm_model = SVC(kernel='linear',max_iter=500, random_state=42)
svm_model.fit(X_train_scaled, y_train)  # Use scaled X_train

# Predictions on scaled data
y_train_pred = svm_model.predict(X_train_scaled)
y_val_pred = svm_model.predict(X_val_scaled)
y_test_pred = svm_model.predict(X_test_scaled)

# Evaluate model performance
train_mse = mean_squared_error(y_train, y_train_pred)
val_mse = mean_squared_error(y_val, y_val_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

train_r2 = r2_score(y_train, y_train_pred)
val_r2 = r2_score(y_val, y_val_pred)
test_r2 = r2_score(y_test, y_test_pred)

train_acc = accuracy_score(y_train, y_train_pred)
val_acc = accuracy_score(y_val, y_val_pred)
test_acc = accuracy_score(y_test, y_test_pred)

# Print results
print(f"Train MSE: {train_mse:.4f}, R²: {train_r2:.4f}, Accuracy: {train_acc:.4f}")
print(f"Validation MSE: {val_mse:.4f}, R²: {val_r2:.4f}, Accuracy: {val_acc:.4f}")
print(f"Test MSE: {test_mse:.4f}, R²: {test_r2:.4f}, Accuracy: {test_acc:.4f}")











# Define independent variables (features)
features = ['Term', 'NoEmp', 'NewExist', 'CreateJob', 'RetainedJob', 'DisbursementGross', 'ChgOffPrinGr', 'SBA_Appv', 'UrbanRural_binary', 'MIS_Status_Binary', 
            'IsFranchise', 'LowDoc_binary', 'RevLineCr_binary',
            'Sector_Accommodation & Food', 'Sector_Admin & Waste Mgmt',
            'Sector_Agriculture', 'Sector_Arts & Recreation', 'Sector_Construction',
            'Sector_Education', 'Sector_Finance', 'Sector_Healthcare',
            'Sector_Information', 'Sector_Management', 'Sector_Manufacturing',
            'Sector_Mining', 'Sector_Other Services', 'Sector_Professional Services',
            'Sector_Public Admin', 'Sector_Real Estate', 'Sector_Retail Trade',
            'Sector_Transportation', 'Sector_Unknown', 'Sector_Utilities',
            'Sector_Wholesale Trade']

# Define target variable
X = df[features]
y = df['ChgOffDate_binary']

# Split data into 70% train, 10% validation, 20% test
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.20, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, random_state=42)

# Standardize the features (Logistic Regression performs better with normalized data)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Train Logistic Regression model
log_model = LogisticRegression()
log_model.fit(X_train_scaled, y_train)

# Predictions
y_train_pred = log_model.predict(X_train_scaled)
y_val_pred = log_model.predict(X_val_scaled)
y_test_pred = log_model.predict(X_test_scaled)

# Evaluate model performance
train_acc = accuracy_score(y_train, y_train_pred)
val_acc = accuracy_score(y_val, y_val_pred)
test_acc = accuracy_score(y_test, y_test_pred)

# Print results
print(f"Train Accuracy: {train_acc:.4f}")
print(f"Validation Accuracy: {val_acc:.4f}")
print(f"Test Accuracy: {test_acc:.4f}")






# Train Random Forest model using the already split data
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)  # Use unscaled X_train

# Predictions
y_train_pred = rf_model.predict(X_train)
y_val_pred = rf_model.predict(X_val)
y_test_pred = rf_model.predict(X_test)

# Evaluate model performance
train_acc = accuracy_score(y_train, y_train_pred)
val_acc = accuracy_score(y_val, y_val_pred)
test_acc = accuracy_score(y_test, y_test_pred)

train_precision = precision_score(y_train, y_train_pred)
val_precision = precision_score(y_val, y_val_pred)
test_precision = precision_score(y_test, y_test_pred)

train_recall = recall_score(y_train, y_train_pred)
val_recall = recall_score(y_val, y_val_pred)
test_recall = recall_score(y_test, y_test_pred)

train_f1 = f1_score(y_train, y_train_pred)
val_f1 = f1_score(y_val, y_val_pred)
test_f1 = f1_score(y_test, y_test_pred)

# Print results
print(f"Train Accuracy: {train_acc:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}")
print(f"Validation Accuracy: {val_acc:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}")
print(f"Test Accuracy: {test_acc:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}")

# Confusion Matrix
print("Test Confusion Matrix:")
print(confusion_matrix(y_test, y_test_pred))





# Train XGBoost model (using already split data)
xgb_model = XGBClassifier(n_estimators=50, random_state=42, eval_metric="logloss")
xgb_model.fit(X_train, y_train)

# Predictions
y_train_pred = xgb_model.predict(X_train)
y_val_pred = xgb_model.predict(X_val)
y_test_pred = xgb_model.predict(X_test)

# Evaluate model performance
train_acc = accuracy_score(y_train, y_train_pred)
val_acc = accuracy_score(y_val, y_val_pred)
test_acc = accuracy_score(y_test, y_test_pred)

train_precision = precision_score(y_train, y_train_pred)
val_precision = precision_score(y_val, y_val_pred)
test_precision = precision_score(y_test, y_test_pred)

train_recall = recall_score(y_train, y_train_pred)
val_recall = recall_score(y_val, y_val_pred)
test_recall = recall_score(y_test, y_test_pred)

train_f1 = f1_score(y_train, y_train_pred)
val_f1 = f1_score(y_val, y_val_pred)
test_f1 = f1_score(y_test, y_test_pred)

# Print results
print(f"Train Accuracy: {train_acc:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}")
print(f"Validation Accuracy: {val_acc:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}")
print(f"Test Accuracy: {test_acc:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}")

# Confusion Matrix
print("Test Confusion Matrix:")
print(confusion_matrix(y_test, y_test_pred))








#split the data into training, test, and validation set 
#from sklearn.model_selection import train_test_split

# MIS_Status_Binary' is the target column
#X = df.drop('MIS_Status_Binary', axis=1)  # Features
#y = df['MIS_Status_Binary']  # Target variable

# split the data into 80% train+validation and 20% test
#X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

# split the train+validation set into 87.5% train and 12.5% validation (to get 70% train and 10%)
#X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, random_state=42)

# print the shapes of the splits to verify
#print(f"Training set size: {X_train.shape[0]} samples")
#print(f"Validation set size: {X_val.shape[0]} samples")
#print(f"Test set size: {X_test.shape[0]} samples")



#B. Binary Target 



