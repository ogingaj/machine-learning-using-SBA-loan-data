# Core Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.ticker as mtick
import seaborn as sns

# Preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split

# Models
from sklearn.svm import SVC, LinearSVC, LinearSVR
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from xgboost import XGBClassifier, XGBRegressor

# Evaluation Metrics
from sklearn.metrics import (
    mean_squared_error,
    r2_score,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    ConfusionMatrixDisplay
)

# Pipeline
from sklearn.pipeline import make_pipeline



#read pre-processed data 
df = pd.read_csv(r"C:\Users\Ochieng' Oginga\Documents\Post_S\Spring_2025\Data_Science_II\Project\Data\Oginga_Stage4.csv")

#show head 
df.head()





df.shape








#get description of the data 
df.describe(include='all')






#get summary of the numeric variables 
df.describe()





#summary of categorical variables 
df.describe(include=[object])








#create a histogram for bumeric variables with bins of 20 
df.hist(figsize=(12, 10), bins=20)





 # Ensure ApprovalFY is numeric
df['ApprovalFY'] = pd.to_numeric(df['ApprovalFY'], errors='coerce')

# Drop any NaN values that might have been introduced during conversion
df = df.dropna(subset=['ApprovalFY'])

# Aggregate and plot
df.groupby('ApprovalFY')['GrAppv'].sum().plot(kind='bar', figsize=(10, 5))

plt.xlabel("Approval Fiscal Year")
plt.ylabel("Approved Gross ($)")
plt.title("Total Approved Gross by ApprovalFY")
plt.xticks(rotation=45)
plt.show()








# Extract the last two digits (year) from DisbursementDate and convert to integer
df['DisbursementYear'] = df['DisbursementDate'].str[-2:].astype(int)

# Fix potential century issue: Convert 00-23 to 2000-2023, and 80-99 to 1980-1999
df['DisbursementYear'] = df['DisbursementYear'].apply(lambda x: x + 2000 if x < 24 else x + 1900)

# Aggregate total disbursement by year
yearly_disbursement = df.groupby('DisbursementYear')['DisbursementGross'].sum()

# Plot
fig, ax = plt.subplots(figsize=(10, 5))
yearly_disbursement.plot(kind='bar', ax=ax)

# Formatting
ax.set_xlabel("Disbursement Year")
ax.set_ylabel("Total Loan Disbursement ($)")
ax.set_title("Total Loan Disbursement by Year")
ax.yaxis.set_major_formatter(mtick.StrMethodFormatter('${x:,.0f}'))  # Format as dollars
plt.xticks(rotation=45)

plt.show()








#box plotting
plt.figure(figsize=(12, 6))
sns.boxplot(data=df.select_dtypes(include=['number']))
plt.xticks(rotation=90)
plt.show()









#check columns 
df.columns





df.shape








#set features and target 
features = ['Term', 'NoEmp', 'NewExist', 'CreateJob', 'RetainedJob', 'DisbursementGross', 'ChgOffPrinGr', 'SBA_Appv', 
            'ChgOffDate_binary', 'UrbanRural_binary', 'MIS_Status_Binary', 
            'IsFranchise', 'LowDoc_binary', 'RevLineCr_binary',
            'Sector_Accommodation & Food', 'Sector_Admin & Waste Mgmt',
            'Sector_Agriculture', 'Sector_Arts & Recreation', 'Sector_Construction',
            'Sector_Education', 'Sector_Finance', 'Sector_Healthcare',
            'Sector_Information', 'Sector_Management', 'Sector_Manufacturing',
            'Sector_Mining', 'Sector_Other Services', 'Sector_Professional Services',
            'Sector_Public Admin', 'Sector_Real Estate', 'Sector_Retail Trade',
            'Sector_Transportation', 'Sector_Unknown', 'Sector_Utilities',
            'Sector_Wholesale Trade']

X = df[features]
y = df['GrAppv']

#Split into 80% train+val, 20% test
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

#Split train_val into 87.5% train, 12.5% val ( to achieve 70% train, 10% val)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, random_state=42)





#Scale features for standardization 
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Fit Linear Regression Model
lin_reg = LinearRegression()
lin_reg.fit(X_train_scaled, y_train)

# Predict
y_train_pred = lin_reg.predict(X_train_scaled)
y_val_pred = lin_reg.predict(X_val_scaled)
y_test_pred = lin_reg.predict(X_test_scaled)

# Evaluate
def evaluate(y_true, y_pred, label=""):
    mse = mean_squared_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    print(f"{label} MSE: {mse:.2f}, R²: {r2:.4f}")

evaluate(y_train, y_train_pred, "Train")
evaluate(y_val, y_val_pred, "Validation")
evaluate(y_test, y_test_pred, "Test")

# Scatter plot of actual vs predicted
plt.figure(figsize=(6,6))
sns.scatterplot(x=y_test, y=y_test_pred)
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title("Actual vs Predicted Values")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # diagonal line
plt.show()





# Train Random Forest Regressor with scaled data
rf_model = RandomForestRegressor(
    n_estimators=100,
    max_depth=10,
    n_jobs=-1,
    random_state=42
)
# Use scaled features
rf_model.fit(X_train_scaled, y_train)  

# Predictions
y_train_pred_rf = rf_model.predict(X_train_scaled)
y_val_pred_rf = rf_model.predict(X_val_scaled)
y_test_pred_rf = rf_model.predict(X_test_scaled)

# Evaluate model performance
train_mse_rf = mean_squared_error(y_train, y_train_pred_rf)
val_mse_rf = mean_squared_error(y_val, y_val_pred_rf)
test_mse_rf = mean_squared_error(y_test, y_test_pred_rf)

train_r2_rf = r2_score(y_train, y_train_pred_rf)
val_r2_rf = r2_score(y_val, y_val_pred_rf)
test_r2_rf = r2_score(y_test, y_test_pred_rf)

# Print results
print(f"Random Forest Train MSE: {train_mse_rf:.4f}, R²: {train_r2_rf:.4f}")
print(f"Random Forest Validation MSE: {val_mse_rf:.4f}, R²: {val_r2_rf:.4f}")
print(f"Random Forest Test MSE: {test_mse_rf:.4f}, R²: {test_r2_rf:.4f}")







# Train XGBoost Regressor
xgb_model = XGBRegressor(n_estimators=50, random_state=42, eval_metric="rmse", n_jobs=-1)
xgb_model.fit(X_train, y_train)

# Predictions
y_train_pred = xgb_model.predict(X_train)
y_val_pred = xgb_model.predict(X_val)
y_test_pred = xgb_model.predict(X_test)

# Evaluate model performance
train_mse = mean_squared_error(y_train, y_train_pred)
val_mse = mean_squared_error(y_val, y_val_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

train_r2 = r2_score(y_train, y_train_pred)
val_r2 = r2_score(y_val, y_val_pred)
test_r2 = r2_score(y_test, y_test_pred)

# Print results
print(f"Train MSE: {train_mse:.4f}, R²: {train_r2:.4f}")
print(f"Validation MSE: {val_mse:.4f}, R²: {val_r2:.4f}")
print(f"Test MSE: {test_mse:.4f}, R²: {test_r2:.4f}")






# Create a pipeline with StandardScaler, PCA, and LinearSVR
svr_pipeline = make_pipeline(
    StandardScaler(),
    PCA(n_components=0.95),  # Retain 95% of variance
    LinearSVR(random_state=42, max_iter=10000)
)

# Fit model on training data
svr_pipeline.fit(X_train, y_train)

# Predictions
y_train_pred = svr_pipeline.predict(X_train)
y_val_pred = svr_pipeline.predict(X_val)
y_test_pred = svr_pipeline.predict(X_test)

# Evaluate model performance
train_mse = mean_squared_error(y_train, y_train_pred)
val_mse = mean_squared_error(y_val, y_val_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

train_r2 = r2_score(y_train, y_train_pred)
val_r2 = r2_score(y_val, y_val_pred)
test_r2 = r2_score(y_test, y_test_pred)

# Print results
print(f"Train MSE: {train_mse:.4f}, R²: {train_r2:.4f}")
print(f"Validation MSE: {val_mse:.4f}, R²: {val_r2:.4f}")
print(f"Test MSE: {test_mse:.4f}, R²: {test_r2:.4f}")






# Create a dictionary of results
results_apprv = {
    'Model': ['Linear Regression', 'Random Forest', 'XGBoost', 'Linear SVM (10%)'],
    'Train MSE': [1989391154.14, 129422729.8295, 1075209796.1439, 21029628772.2606],
    'Train R²': [0.9783, 0.9986, 0.9883, 0.7705],
    'Val MSE': [1794591167.47, 246892299.9305, 1834541327.1724, 20930006005.8791],
    'Val R²': [0.9802, 0.9973, 0.9798, 0.7696],
    'Test MSE': [1981584865.49, 259425575.0244, 1923854431.4823, 21209814478.5109],
    'Test R²': [0.9785, 0.9972, 0.9791, 0.7695]
}

# Convert to DataFrame
results_df = pd.DataFrame(results_apprv)

# Display it
results_df


# Predictions for each model
y_test_pred_lin = lin_reg.predict(X_test_scaled)          # Linear Regression (trained on scaled)
y_test_pred_rf = rf_model.predict(X_test_scaled)          # Random Forest (trained on scaled)
y_test_pred_xgb = xgb_model.predict(X_test)               # XGBoost (trained on unscaled)
y_test_pred_svr = svr_pipeline.predict(X_test)            # SVM pipeline handles scaling

# Store predictions in a DataFrame
df_plot_multi = pd.DataFrame({
    'Actual': y_test.values,
    'LinearReg': y_test_pred_lin,
    'RandomForest': y_test_pred_rf,
    'XGBoost': y_test_pred_xgb,
    'SVM': y_test_pred_svr
})

# Sort and bin by actual values
df_plot_multi = df_plot_multi.sort_values(by='Actual').reset_index(drop=True)
df_plot_multi['bin'] = df_plot_multi.index // 100  # 100 observations per bin

# Group by bin and compute mean
df_binned_multi = df_plot_multi.groupby('bin').mean()

# Plot
plt.figure(figsize=(10, 6))
plt.plot(df_binned_multi['Actual'], df_binned_multi['LinearReg'], label='Linear Regression', linestyle='-', marker='o')
plt.plot(df_binned_multi['Actual'], df_binned_multi['RandomForest'], label='Random Forest', linestyle='-', marker='x')
plt.plot(df_binned_multi['Actual'], df_binned_multi['XGBoost'], label='XGBoost', linestyle='-', marker='^')
plt.plot(df_binned_multi['Actual'], df_binned_multi['SVM'], label='SVM', linestyle='-', marker='s')
plt.plot(df_binned_multi['Actual'], df_binned_multi['Actual'], linestyle='--', color='black', label='Perfect Prediction')

# Labels and title
plt.xlabel('Average Actual Gross Approval ($)')
plt.ylabel('Average Predicted Gross Approval ($)')
plt.title('Comparison: Binned Predicted vs Actual for Multiple Models')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()















df.shape


# Define features and target variable
features = ['NoEmp', 'NewExist', 'DisbursementGross', 'GrAppv', 'SBA_Appv', 'UrbanRural_binary', 'MIS_Status_Binary', 
            'IsFranchise', 'LowDoc_binary', 'RevLineCr_binary',
            'Sector_Accommodation & Food', 'Sector_Admin & Waste Mgmt',
            'Sector_Agriculture', 'Sector_Arts & Recreation', 'Sector_Construction',
            'Sector_Education', 'Sector_Finance', 'Sector_Healthcare',
            'Sector_Information', 'Sector_Management', 'Sector_Manufacturing',
            'Sector_Mining', 'Sector_Other Services', 'Sector_Professional Services',
            'Sector_Public Admin', 'Sector_Real Estate', 'Sector_Retail Trade',
            'Sector_Transportation', 'Sector_Unknown', 'Sector_Utilities',
            'Sector_Wholesale Trade']

X = df[features]
y = df['ChgOffPrinGr']

# Split data into 70% train, 10% validation, 20% test
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.20, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, random_state=42)






# Standardize the features (Linear Regression performs better with normalized data)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Train Linear Regression model
lr_model = LinearRegression()
lr_model.fit(X_train_scaled, y_train)

# Predictions
y_train_pred = lr_model.predict(X_train_scaled)
y_val_pred = lr_model.predict(X_val_scaled)
y_test_pred = lr_model.predict(X_test_scaled)

# Evaluate model performance
train_mse = mean_squared_error(y_train, y_train_pred)
val_mse = mean_squared_error(y_val, y_val_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

train_r2 = r2_score(y_train, y_train_pred)
val_r2 = r2_score(y_val, y_val_pred)
test_r2 = r2_score(y_test, y_test_pred)

# Print results
print(f"Train MSE: {train_mse:.4f}, R²: {train_r2:.4f}")
print(f"Validation MSE: {val_mse:.4f}, R²: {val_r2:.4f}")
print(f"Test MSE: {test_mse:.4f}, R²: {test_r2:.4f}")





# Train Random Forest model
rf_model = RandomForestRegressor(n_estimators=10, random_state=42)
rf_model.fit(X_train, y_train)

# Predictions
y_train_pred = rf_model.predict(X_train)
y_val_pred = rf_model.predict(X_val)
y_test_pred = rf_model.predict(X_test)

# Evaluate performance
train_mse = mean_squared_error(y_train, y_train_pred)
val_mse = mean_squared_error(y_val, y_val_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

train_rmse = np.sqrt(train_mse)
val_rmse = np.sqrt(val_mse)
test_rmse = np.sqrt(test_mse)

train_r2 = r2_score(y_train, y_train_pred)
val_r2 = r2_score(y_val, y_val_pred)
test_r2 = r2_score(y_test, y_test_pred)

# Print results
print(f"Train RMSE: {train_rmse:.4f}, R²: {train_r2:.4f}")
print(f"Validation RMSE: {val_rmse:.4f}, R²: {val_r2:.4f}")
print(f"Test RMSE: {test_rmse:.4f}, R²: {test_r2:.4f}")






# Train XGBoost model (regression)
xgb_model = XGBRegressor(n_estimators=50, random_state=42)
xgb_model.fit(X_train, y_train)

# Predictions
y_train_pred = xgb_model.predict(X_train)
y_val_pred = xgb_model.predict(X_val)
y_test_pred = xgb_model.predict(X_test)

# Evaluate model performance
train_mse = mean_squared_error(y_train, y_train_pred)
val_mse = mean_squared_error(y_val, y_val_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

train_r2 = r2_score(y_train, y_train_pred)
val_r2 = r2_score(y_val, y_val_pred)
test_r2 = r2_score(y_test, y_test_pred)

# Print results
print(f"Train MSE: {train_mse:.4f}, R²: {train_r2:.4f}")
print(f"Validation MSE: {val_mse:.4f}, R²: {val_r2:.4f}")
print(f"Test MSE: {test_mse:.4f}, R²: {test_r2:.4f}")








# Subsample 10% of the training data
X_train_sub, _, y_train_sub, _ = train_test_split(X_train, y_train, train_size=0.02, random_state=42, shuffle=True)

# Standardize the features
scaler = StandardScaler()
X_train_sub_scaled = scaler.fit_transform(X_train_sub)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Train Linear SVM model
svm_model = LinearSVC(max_iter=1000, random_state=42)
svm_model.fit(X_train_sub_scaled, y_train_sub)

# Predictions
y_train_pred = svm_model.predict(X_train_sub_scaled)
y_val_pred = svm_model.predict(X_val_scaled)
y_test_pred = svm_model.predict(X_test_scaled)

# Evaluate model performance
train_mse = mean_squared_error(y_train_sub, y_train_pred)
val_mse = mean_squared_error(y_val, y_val_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

train_r2 = r2_score(y_train_sub, y_train_pred)
val_r2 = r2_score(y_val, y_val_pred)
test_r2 = r2_score(y_test, y_test_pred)

# Print results
print(f"Train MSE: {train_mse:.4f}, R²: {train_r2:.4f}")
print(f"Validation MSE: {val_mse:.4f}, R²: {val_r2:.4f}")
print(f"Test MSE: {test_mse:.4f}, R²: {test_r2:.4f}")





# Create a dictionary of results
results = {
    'Model': ['Linear Regression', 'Random Forest', 'XGBoost', 'Linear SVM (10%)'],
    'Train MSE': [4222554684.93, 13122.5781, 238522649.4237, 10889396403.4802],
    'Train R²': [0.2544, 0.9696, 0.9579, -0.7378],
    'Val MSE': [3914276643.5948, 35667.8978, 1185831992.6504, 11950641000.6477],
    'Val R²': [0.2850, 0.7676, 0.7834, -1.0756],
    'Test MSE': [4280889619.9658, 32811.5325, 1090246310.4710, 11890937938.6569],
    'Test R²': [0.2639, 0.8149, 0.8125, -1.1637]
}

# Convert to DataFrame
results_df = pd.DataFrame(results)

# Display it
results_df









# Get predictions for each model
y_test_pred_rf = rf_model.predict(X_test)                # RandomForestRegressor
y_test_pred_xgb = xgb_model.predict(X_test)              # XGBoost Regressor
y_test_pred_svm = svm_model.predict(X_test_scaled)       # SVM predictions using scaled data
y_test_pred_lr = lr_model.predict(X_test_scaled)         # Linear Regression predictions using scaled data

# Store predictions in a DataFrame
df_plot_multi = pd.DataFrame({
    'Actual': y_test.values,
    'RandomForest': y_test_pred_rf,
    'XGBoost': y_test_pred_xgb,
    'SVM': y_test_pred_svm,
    'LinearReg': y_test_pred_lr
})

# Sort and bin by actual values
df_plot_multi = df_plot_multi.sort_values(by='Actual').reset_index(drop=True)
# Adjust bin size based on your dataset size
bin_size = max(1, len(df_plot_multi) // 100)  # Ensure at least 1 observation per bin
df_plot_multi['bin'] = df_plot_multi.index // bin_size

# Group by bin and compute mean
df_binned_multi = df_plot_multi.groupby('bin').mean()

# Plot
plt.figure(figsize=(12, 7))
plt.plot(df_binned_multi['Actual'], df_binned_multi['RandomForest'], label='Random Forest', linestyle='-', marker='x')
plt.plot(df_binned_multi['Actual'], df_binned_multi['XGBoost'], label='XGBoost', linestyle='-', marker='^')
plt.plot(df_binned_multi['Actual'], df_binned_multi['SVM'], label='SVM', linestyle='-', marker='s')
plt.plot(df_binned_multi['Actual'], df_binned_multi['LinearReg'], label='Linear Regression', linestyle='-', marker='o')

# Perfect prediction line
plt.plot(df_binned_multi['Actual'], df_binned_multi['Actual'], linestyle='--', color='black', label='Perfect Prediction')

# Labels and title
plt.xlabel('Average Actual ChgOffPrinGr ($)')
plt.ylabel('Average Predicted ChgOffPrinGr ($)')
plt.title('Comparison: Binned Predicted vs Actual for Multiple Models')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()





df.shape


# Take 100% sample of the dataset
#df = df.sample(frac=1, random_state=42)

# Define features and target variable
features = ["DisbursementGross", "Term", 'NoEmp', "NewExist", "UrbanRural_binary", 'RetainedJob', 'ChgOffPrinGr', 'SBA_Appv', 
            'ChgOffDate_binary', 'MIS_Status_Binary', 
            'IsFranchise', 'LowDoc_binary', 'RevLineCr_binary',
            'Sector_Accommodation & Food', 'Sector_Admin & Waste Mgmt',
            'Sector_Agriculture', 'Sector_Arts & Recreation', 'Sector_Construction',
            'Sector_Education', 'Sector_Finance', 'Sector_Healthcare',
            'Sector_Information', 'Sector_Management', 'Sector_Manufacturing',
            'Sector_Mining', 'Sector_Other Services', 'Sector_Professional Services',
            'Sector_Public Admin', 'Sector_Real Estate', 'Sector_Retail Trade',
            'Sector_Transportation', 'Sector_Unknown', 'Sector_Utilities',
            'Sector_Wholesale Trade']
X = df[features]
y = df['CreateJob']

# Split data into 70% train, 10% validation, 20% test
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.20, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, random_state=42)


#see the split
print(f"Train set size: {X_train.shape[0]}")
print(f"Validation set size: {X_val.shape[0]}")
print(f"Test set size: {X_test.shape[0]}")






# Train Linear Regression model
lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)

# Predictions
y_train_pred_lin = lin_reg.predict(X_train)
y_val_pred_lin = lin_reg.predict(X_val)
y_test_pred_lin = lin_reg.predict(X_test)

# Evaluate model performance
train_mse_lin = mean_squared_error(y_train, y_train_pred_lin)
val_mse_lin = mean_squared_error(y_val, y_val_pred_lin)
test_mse_lin = mean_squared_error(y_test, y_test_pred_lin)

train_r2_lin = r2_score(y_train, y_train_pred_lin)
val_r2_lin = r2_score(y_val, y_val_pred_lin)
test_r2_lin = r2_score(y_test, y_test_pred_lin)

# Print results
print(f"Linear Regression:")
print(f"Train MSE: {train_mse_lin:.4f}, R²: {train_r2_lin:.4f}")
print(f"Validation MSE: {val_mse_lin:.4f}, R²: {val_r2_lin:.4f}")
print(f"Test MSE: {test_mse_lin:.4f}, R²: {test_r2_lin:.4f}")





# Train Random Forest model - using regressor now
rf_model = RandomForestRegressor(n_estimators=10, random_state=42)
rf_model.fit(X_train, y_train)

# Predictions
y_train_pred = rf_model.predict(X_train)
y_val_pred = rf_model.predict(X_val)
y_test_pred = rf_model.predict(X_test)

# Evaluate model performance with regression metrics
train_mse = mean_squared_error(y_train, y_train_pred)
val_mse = mean_squared_error(y_val, y_val_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

train_r2 = r2_score(y_train, y_train_pred)
val_r2 = r2_score(y_val, y_val_pred)
test_r2 = r2_score(y_test, y_test_pred)

# Print results
print(f"Train MSE: {train_mse:.4f}, R²: {train_r2:.4f}")
print(f"Validation MSE: {val_mse:.4f}, R²: {val_r2:.4f}")
print(f"Test MSE: {test_mse:.4f}, R²: {test_r2:.4f}")





# Convert target variable to integer type (if it's categorical)
y_train = y_train.astype(int)
y_test = y_test.astype(int)

# Check unique classes in train and test sets
print(np.unique(y_train))
print(np.unique(y_test))


# Train XGBoost model (regression)
xgb_model = XGBRegressor(n_estimators=50, random_state=42)
xgb_model.fit(X_train, y_train)

# Predictions
y_train_pred = xgb_model.predict(X_train)
y_val_pred = xgb_model.predict(X_val)
y_test_pred = xgb_model.predict(X_test)

# Evaluate model performance
train_mse = mean_squared_error(y_train, y_train_pred)
val_mse = mean_squared_error(y_val, y_val_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

train_r2 = r2_score(y_train, y_train_pred)
val_r2 = r2_score(y_val, y_val_pred)
test_r2 = r2_score(y_test, y_test_pred)

# Print results
print(f"Train MSE: {train_mse:.4f}, R²: {train_r2:.4f}")
print(f"Validation MSE: {val_mse:.4f}, R²: {val_r2:.4f}")
print(f"Test MSE: {test_mse:.4f}, R²: {test_r2:.4f}")


df.shape 





# Subsample 2% of the training data
X_train_sub2, _, y_train_sub2, _ = train_test_split(X_train, y_train, train_size=0.02, random_state=42, shuffle=True)

# Standardize the features
scaler = StandardScaler()
X_train_sub2_scaled = scaler.fit_transform(X_train_sub2)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Train Linear SVM model
svm_model = LinearSVC(max_iter=1000, random_state=42)
svm_model.fit(X_train_sub2_scaled, y_train_sub2)

# Predictions
y_train_pred = svm_model.predict(X_train_sub2_scaled)
y_val_pred = svm_model.predict(X_val_scaled)
y_test_pred = svm_model.predict(X_test_scaled)

# Evaluate model performance
train_mse = mean_squared_error(y_train_sub2, y_train_pred)
val_mse = mean_squared_error(y_val, y_val_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

train_r2 = r2_score(y_train_sub2, y_train_pred)
val_r2 = r2_score(y_val, y_val_pred)
test_r2 = r2_score(y_test, y_test_pred)

# Print results
print(f"Train MSE: {train_mse:.4f}, R²: {train_r2:.4f}")
print(f"Validation MSE: {val_mse:.4f}, R²: {val_r2:.4f}")
print(f"Test MSE: {test_mse:.4f}, R²: {test_r2:.4f}")






# Create a dictionary of results
results_jobs = {
    'Model': ['Linear Regression', 'Random Forest', 'XGBoost', 'Linear SVM (10%)'],
    'Train MSE': [143.6146, 26.2989, 87.5700, 10889396403.4802],
    'Train R²': [0.0519, 0.8264, 0.4219, 0.0350],
    'Val MSE': [102.7379, 105.9432, 85.3770, 11950641000.6477],
    'Val R²': [-0.0590, -0.0921, 0.1199, -0.1586],
    'Test MSE': [360.3609, 386.1715, 369.3773, 11890937938.6569],
    'Test R²': [0.0265, -0.0432, 0.0022, -0.0101]
}

# Convert to DataFrame
results_df_jobs = pd.DataFrame(results_jobs)

# Display it
results_df_jobs







# Predictions for each model
y_test_pred_rf = rf_model.predict(X_test)      # Random Forest Regressor
y_test_pred_xgb = xgb_model.predict(X_test)    # XGBoost Regressor
y_test_pred_lin = lin_reg.predict(X_test)      # Linear Regression

# Store predictions in a DataFrame
df_plot_multi = pd.DataFrame({
    'Actual': y_test.values,
    'RandomForest': y_test_pred_rf,
    'XGBoost': y_test_pred_xgb,
    'LinearReg': y_test_pred_lin
})

# Sort and bin by actual values
df_plot_multi = df_plot_multi.sort_values(by='Actual').reset_index(drop=True)
df_plot_multi['bin'] = df_plot_multi.index // 100  # 100 observations per bin

# Group by bin and compute mean
df_binned_multi = df_plot_multi.groupby('bin').mean()

# Plot
plt.figure(figsize=(10, 6))
plt.plot(df_binned_multi['Actual'], df_binned_multi['RandomForest'], label='Random Forest', linestyle='-', marker='x')
plt.plot(df_binned_multi['Actual'], df_binned_multi['XGBoost'], label='XGBoost', linestyle='-', marker='^')
plt.plot(df_binned_multi['Actual'], df_binned_multi['LinearReg'], label='Linear Regression', linestyle='-', marker='o')
plt.plot(df_binned_multi['Actual'], df_binned_multi['Actual'], linestyle='--', color='black', label='Perfect Prediction')

# Labels and title
plt.xlabel('Average Actual CreateJob')
plt.ylabel('Average Predicted CreateJob')
plt.title('Comparison: Binned Predicted vs Actual for Multiple Models')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()








# Define independent variables (features)
features = ['Term', 'NoEmp', 'NewExist', 'CreateJob', 'RetainedJob', 'DisbursementGross', 'ChgOffPrinGr', 'SBA_Appv', 
            'ChgOffDate_binary', 'UrbanRural_binary', 
            'IsFranchise', 'LowDoc_binary', 'RevLineCr_binary',
            'Sector_Accommodation & Food', 'Sector_Admin & Waste Mgmt',
            'Sector_Agriculture', 'Sector_Arts & Recreation', 'Sector_Construction',
            'Sector_Education', 'Sector_Finance', 'Sector_Healthcare',
            'Sector_Information', 'Sector_Management', 'Sector_Manufacturing',
            'Sector_Mining', 'Sector_Other Services', 'Sector_Professional Services',
            'Sector_Public Admin', 'Sector_Real Estate', 'Sector_Retail Trade',
            'Sector_Transportation', 'Sector_Unknown', 'Sector_Utilities',
            'Sector_Wholesale Trade']

# Define target variable
X = df[features]
y = df['MIS_Status_Binary']

# Split data into 70% train, 10% validation, 20% test
#first get 20% to be test data 
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.20, random_state=42)
#then split the train data further to have validation set 
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, random_state=42)


#see the split
print(f"Train set size: {X_train.shape[0]}")
print(f"Validation set size: {X_val.shape[0]}")
print(f"Test set size: {X_test.shape[0]}")





# Standardize the features (Logistic Regression performs better with normalized data)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Train Logistic Regression model
log_model = LogisticRegression()
log_model.fit(X_train_scaled, y_train)

# Predictions
y_train_pred = log_model.predict(X_train_scaled)
y_val_pred = log_model.predict(X_val_scaled)
y_test_pred = log_model.predict(X_test_scaled)

# Evaluate model performance
train_acc = accuracy_score(y_train, y_train_pred)
val_acc = accuracy_score(y_val, y_val_pred)
test_acc = accuracy_score(y_test, y_test_pred)

train_precision = precision_score(y_train, y_train_pred)
val_precision = precision_score(y_val, y_val_pred)
test_precision = precision_score(y_test, y_test_pred)

train_recall = recall_score(y_train, y_train_pred)
val_recall = recall_score(y_val, y_val_pred)
test_recall = recall_score(y_test, y_test_pred)

train_f1 = f1_score(y_train, y_train_pred)
val_f1 = f1_score(y_val, y_val_pred)
test_f1 = f1_score(y_test, y_test_pred)

# Print results
print(f"Train Accuracy: {train_acc:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}")
print(f"Validation Accuracy: {val_acc:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}")
print(f"Test Accuracy: {test_acc:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}")






# Compute confusion matrix
cm = confusion_matrix(y_test, y_test_pred)

# Plot confusion matrix
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - Test Set")
plt.show()





# Train Random Forest model with a different variable name
rf_model_MIS = RandomForestClassifier(n_estimators=10, random_state=42)
rf_model_MIS.fit(X_train, y_train)  # Use unscaled X_train
# Predictions
y_train_pred = rf_model_MIS.predict(X_train)
y_val_pred = rf_model_MIS.predict(X_val)
y_test_pred = rf_model_MIS.predict(X_test)

# Evaluate model performance
train_acc = accuracy_score(y_train, y_train_pred)
val_acc = accuracy_score(y_val, y_val_pred)
test_acc = accuracy_score(y_test, y_test_pred)

train_precision = precision_score(y_train, y_train_pred)
val_precision = precision_score(y_val, y_val_pred)
test_precision = precision_score(y_test, y_test_pred)

train_recall = recall_score(y_train, y_train_pred)
val_recall = recall_score(y_val, y_val_pred)
test_recall = recall_score(y_test, y_test_pred)

train_f1 = f1_score(y_train, y_train_pred)
val_f1 = f1_score(y_val, y_val_pred)
test_f1 = f1_score(y_test, y_test_pred)

# Print results
print(f"Train Accuracy: {train_acc:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}")
print(f"Validation Accuracy: {val_acc:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}")
print(f"Test Accuracy: {test_acc:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}")



# Compute confusion matrix
cm = confusion_matrix(y_test, y_test_pred)

# Plot confusion matrix
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Greens", cbar=False)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - Random Forest (Test Set)")
plt.show()





# Train XGBoost model
xgb_model = XGBClassifier(n_estimators=50, random_state=42, eval_metric="logloss")
xgb_model.fit(X_train, y_train)

# Predictions
y_train_pred = xgb_model.predict(X_train)
y_val_pred = xgb_model.predict(X_val)
y_test_pred = xgb_model.predict(X_test)

# Evaluate model performance
train_acc = accuracy_score(y_train, y_train_pred)
val_acc = accuracy_score(y_val, y_val_pred)
test_acc = accuracy_score(y_test, y_test_pred)

train_precision = precision_score(y_train, y_train_pred)
val_precision = precision_score(y_val, y_val_pred)
test_precision = precision_score(y_test, y_test_pred)

train_recall = recall_score(y_train, y_train_pred)
val_recall = recall_score(y_val, y_val_pred)
test_recall = recall_score(y_test, y_test_pred)

train_f1 = f1_score(y_train, y_train_pred)
val_f1 = f1_score(y_val, y_val_pred)
test_f1 = f1_score(y_test, y_test_pred)

# Print results
print(f"Train Accuracy: {train_acc:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}")
print(f"Validation Accuracy: {val_acc:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}")
print(f"Test Accuracy: {test_acc:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}")



# Compute confusion matrix
cm = confusion_matrix(y_test, y_test_pred)

# Plot confusion matrix
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Purples", cbar=False)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - XGBoost (Test Set)")
plt.show()








# Subsample 2% of the training data
X_train_sub3, _, y_train_sub3, _ = train_test_split(X_train, y_train, train_size=0.02, random_state=42, shuffle=True)

# Scale features
scaler = StandardScaler()
X_train_sub3_scaled = scaler.fit_transform(X_train_sub3)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Train LinearSVC model with scaled data
svm_model = LinearSVC(max_iter=500, random_state=42)
svm_model.fit(X_train_sub3_scaled, y_train_sub3)

# Predictions on scaled data
y_train_pred = svm_model.predict(X_train_sub3_scaled)
y_val_pred = svm_model.predict(X_val_scaled)
y_test_pred = svm_model.predict(X_test_scaled)

# Evaluate model performance
train_mse = mean_squared_error(y_train_sub3, y_train_pred)
val_mse = mean_squared_error(y_val, y_val_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

train_r2 = r2_score(y_train_sub3, y_train_pred)
val_r2 = r2_score(y_val, y_val_pred)
test_r2 = r2_score(y_test, y_test_pred)

train_acc = accuracy_score(y_train_sub3, y_train_pred)
val_acc = accuracy_score(y_val, y_val_pred)
test_acc = accuracy_score(y_test, y_test_pred)

# Print results
print(f"Train MSE: {train_mse:.4f}, R²: {train_r2:.4f}, Accuracy: {train_acc:.4f}")
print(f"Validation MSE: {val_mse:.4f}, R²: {val_r2:.4f}, Accuracy: {val_acc:.4f}")
print(f"Test MSE: {test_mse:.4f}, R²: {test_r2:.4f}, Accuracy: {test_acc:.4f}")





# Compute confusion matrix
cm = confusion_matrix(y_test, y_test_pred)

# Plot confusion matrix
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="BuGn", cbar=False)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - LinearSVC (Test Set)")
plt.show()





#compare model performance 
# Create a dictionary of repayment results
results_repayment = {
    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost', 'Linear SVM'],
    'Train MSE': [0.0070, 0.0016, 0.0070, 0.0065],
    'Train R²': [0.9611, 0.9912, 0.9613, 0.9631],
    'Train Accuracy': [0.9930, 0.9984, 0.9930, 0.9935],
    'Val MSE': [0.0071, 0.0074, 0.0071, 0.0071],
    'Val R²': [0.9608, 0.9587, 0.9608, 0.9607],
    'Val Accuracy': [0.9929, 0.9926, 0.9929, 0.9929],
    'Test MSE': [0.0068, 0.0071, 0.0069, 0.0069],
    'Test R²': [0.9618, 0.9602, 0.9617, 0.9617],
    'Test Accuracy': [0.9932, 0.9929, 0.9931, 0.9931]
}

# Convert to DataFrame
results_df_repayment = pd.DataFrame(results_repayment)

# Display it
results_df_repayment









# Define independent variables (features)
features = ['Term', 'NoEmp', 'NewExist', 'CreateJob', 'RetainedJob', 'DisbursementGross', 
            'ChgOffPrinGr', 'SBA_Appv', 'UrbanRural_binary', 'MIS_Status_Binary', 
            'IsFranchise', 'LowDoc_binary', 'RevLineCr_binary',
            'Sector_Accommodation & Food', 'Sector_Admin & Waste Mgmt',
            'Sector_Agriculture', 'Sector_Arts & Recreation', 'Sector_Construction',
            'Sector_Education', 'Sector_Finance', 'Sector_Healthcare',
            'Sector_Information', 'Sector_Management', 'Sector_Manufacturing',
            'Sector_Mining', 'Sector_Other Services', 'Sector_Professional Services',
            'Sector_Public Admin', 'Sector_Real Estate', 'Sector_Retail Trade',
            'Sector_Transportation', 'Sector_Unknown', 'Sector_Utilities',
            'Sector_Wholesale Trade']

# Define target variable
X = df[features]
y = df['ChgOffDate_binary']

# Split data into 70% train, 10% validation, 20% test
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.20, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, random_state=42)


#see the split
print(f"Train set size: {X_train.shape[0]}")
print(f"Validation set size: {X_val.shape[0]}")
print(f"Test set size: {X_test.shape[0]}")





# Standardize the features (Logistic Regression performs better with normalized data)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Train Logistic Regression model
log_model = LogisticRegression(random_state=42)
log_model.fit(X_train_scaled, y_train)

# Predictions
y_train_pred = log_model.predict(X_train_scaled)
y_val_pred = log_model.predict(X_val_scaled)
y_test_pred = log_model.predict(X_test_scaled)

# Evaluate model performance
train_acc = accuracy_score(y_train, y_train_pred)
val_acc = accuracy_score(y_val, y_val_pred)
test_acc = accuracy_score(y_test, y_test_pred)

train_precision = precision_score(y_train, y_train_pred)
val_precision = precision_score(y_val, y_val_pred)
test_precision = precision_score(y_test, y_test_pred)

train_recall = recall_score(y_train, y_train_pred)
val_recall = recall_score(y_val, y_val_pred)
test_recall = recall_score(y_test, y_test_pred)

train_f1 = f1_score(y_train, y_train_pred)
val_f1 = f1_score(y_val, y_val_pred)
test_f1 = f1_score(y_test, y_test_pred)

# Print results
print(f"Train Accuracy: {train_acc:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}")
print(f"Validation Accuracy: {val_acc:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}")
print(f"Test Accuracy: {test_acc:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}")






# Compute confusion matrix
cm = confusion_matrix(y_test, y_test_pred)

# Plot confusion matrix heatmap
plt.figure(figsize=(5, 4))
sns.heatmap(cm, annot=True, fmt="d", cmap="YlGnBu", cbar=False)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - Logistic Regression (Test Set)")
plt.show()





# Train RandomForestClassifier model using the already split data
rf_model = RandomForestClassifier(n_estimators=10, random_state=42)
rf_model.fit(X_train, y_train)  # Use unscaled X_train

# Predictions
y_train_pred = rf_model.predict(X_train)
y_val_pred = rf_model.predict(X_val)
y_test_pred = rf_model.predict(X_test)

# Evaluate model performance
train_acc = accuracy_score(y_train, y_train_pred)
val_acc = accuracy_score(y_val, y_val_pred)
test_acc = accuracy_score(y_test, y_test_pred)
train_precision = precision_score(y_train, y_train_pred)
val_precision = precision_score(y_val, y_val_pred)
test_precision = precision_score(y_test, y_test_pred)
train_recall = recall_score(y_train, y_train_pred)
val_recall = recall_score(y_val, y_val_pred)
test_recall = recall_score(y_test, y_test_pred)
train_f1 = f1_score(y_train, y_train_pred)
val_f1 = f1_score(y_val, y_val_pred)
test_f1 = f1_score(y_test, y_test_pred)

# Print results
print(f"Train Accuracy: {train_acc:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}")
print(f"Validation Accuracy: {val_acc:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}")
print(f"Test Accuracy: {test_acc:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}")





# Compute confusion matrix
cm = confusion_matrix(y_test, y_test_pred)

# Plot confusion matrix heatmap
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="YlGnBu", cbar=False)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - Random Forest (Test Set)")
plt.show()





# Train XGBoost model (using already split data)
xgb_model = XGBClassifier(n_estimators=50, random_state=42, eval_metric="logloss")
xgb_model.fit(X_train, y_train)

# Predictions
y_train_pred = xgb_model.predict(X_train)
y_val_pred = xgb_model.predict(X_val)
y_test_pred = xgb_model.predict(X_test)

# Evaluate model performance
train_acc = accuracy_score(y_train, y_train_pred)
val_acc = accuracy_score(y_val, y_val_pred)
test_acc = accuracy_score(y_test, y_test_pred)

train_precision = precision_score(y_train, y_train_pred)
val_precision = precision_score(y_val, y_val_pred)
test_precision = precision_score(y_test, y_test_pred)

train_recall = recall_score(y_train, y_train_pred)
val_recall = recall_score(y_val, y_val_pred)
test_recall = recall_score(y_test, y_test_pred)

train_f1 = f1_score(y_train, y_train_pred)
val_f1 = f1_score(y_val, y_val_pred)
test_f1 = f1_score(y_test, y_test_pred)

# Print results
print(f"Train Accuracy: {train_acc:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}")
print(f"Validation Accuracy: {val_acc:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}")
print(f"Test Accuracy: {test_acc:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}")

# Confusion Matrix
print("Test Confusion Matrix:")
print(confusion_matrix(y_test, y_test_pred))





# Compute confusion matrix
cm = confusion_matrix(y_test, y_test_pred)

# Plot confusion matrix heatmap
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="YlGnBu", cbar=False)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - XGBoost (Test Set)")
plt.show()





# Subsample 2% of the training data
X_train_sub4, _, y_train_sub4, _ = train_test_split(X_train, y_train, train_size=0.02, random_state=42, shuffle=True)

# Standardize features
scaler = StandardScaler()
X_train_sub4_scaled = scaler.fit_transform(X_train_sub4)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Train Linear SVM model
svm_model = LinearSVC(max_iter=1000, random_state=42)
svm_model.fit(X_train_sub4_scaled, y_train_sub4)

# Predictions
y_train_pred = svm_model.predict(X_train_sub4_scaled)
y_val_pred = svm_model.predict(X_val_scaled)
y_test_pred = svm_model.predict(X_test_scaled)

# Evaluate model performance
train_acc = accuracy_score(y_train_sub4, y_train_pred)
val_acc = accuracy_score(y_val, y_val_pred)
test_acc = accuracy_score(y_test, y_test_pred)

train_precision = precision_score(y_train_sub4, y_train_pred)
val_precision = precision_score(y_val, y_val_pred)
test_precision = precision_score(y_test, y_test_pred)

train_recall = recall_score(y_train_sub4, y_train_pred)
val_recall = recall_score(y_val, y_val_pred)
test_recall = recall_score(y_test, y_test_pred)

train_f1 = f1_score(y_train_sub4, y_train_pred)
val_f1 = f1_score(y_val, y_val_pred)
test_f1 = f1_score(y_test, y_test_pred)

# Print results
print(f"Train Accuracy: {train_acc:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}")
print(f"Validation Accuracy: {val_acc:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}")
print(f"Test Accuracy: {test_acc:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}")






# Compute confusion matrix
cm = confusion_matrix(y_test, y_test_pred)

# Plot confusion matrix
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="BuGn", cbar=False)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - LinearSVC (Test Set)")
plt.show()




